"""
A3 Node - Content Analysis
ì¡°í•­ë³„ ë‚´ìš© ì¶©ì‹¤ë„ ë¶„ì„ ë° ê°œì„  ì œì•ˆ ìƒì„±
"""

import logging
import time
from datetime import datetime
from typing import Dict, Any, List, Optional

from ..models import ArticleAnalysis, ContentAnalysisResult
from .content_comparator import ContentComparator

logger = logging.getLogger(__name__)


class ContentAnalysisNode:
    """
    Content analysis node (A3).

    Responsibilities:
    1. Load standard article details from A1 matching outputs.
    2. Compare contract content via LLM (ContentComparator).
    3. Draft improvement suggestions (SuggestionGenerator).
    4. Handle special cases (SpecialArticleHandler).
    """
    
    def __init__(
        self,
        knowledge_base_loader,
        azure_client
    ):
        """
        Args:
            knowledge_base_loader: KnowledgeBaseLoader ì¸ìŠ¤í„´ìŠ¤
            azure_client: Azure OpenAI í´ë¼ì´ì–¸íŠ¸
        """
        self.kb_loader = knowledge_base_loader
        self.azure_client = azure_client

        self.content_comparator = ContentComparator(azure_client)

        # TODO: ë‹¤ë¥¸ ì»´í¬ë„ŒíŠ¸ë“¤ ì´ˆê¸°í™”
        # self.suggestion_generator = SuggestionGenerator(azure_client)
        # self.special_handler = SpecialArticleHandler(azure_client)
        
        logger.info("A3 ë…¸ë“œ (Content Analysis) ì´ˆê¸°í™” ì™„ë£Œ")
    
    def analyze_contract(
        self,
        contract_id: str,
        user_contract: Dict[str, Any],
        contract_type: str,
        matching_types: List[str] = None,
        text_weight: float = 0.7,
        title_weight: float = 0.3,
        dense_weight: float = 0.85
    ) -> ContentAnalysisResult:
        """
        ê³„ì•½ì„œ ì „ì²´ ë¶„ì„ (A1 ë§¤ì¹­ ê²°ê³¼ ì°¸ì¡°)

        Args:
            contract_id: ê³„ì•½ì„œ ID
            user_contract: ì‚¬ìš©ì ê³„ì•½ì„œ íŒŒì‹± ê²°ê³¼
            contract_type: ë¶„ë¥˜ëœ ê³„ì•½ ìœ í˜•
            matching_types: ì²˜ë¦¬í•  ë§¤ì¹­ ìœ í˜• (["primary"], ["recovered"], ["primary", "recovered"])
                           Noneì´ë©´ ["primary"] ì‚¬ìš© (ê¸°ë³¸ê°’, í•˜ìœ„ í˜¸í™˜ì„±)
            text_weight: ë³¸ë¬¸ ê°€ì¤‘ì¹˜ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ, A1ì—ì„œ ì´ë¯¸ ë§¤ì¹­ ì™„ë£Œ)
            title_weight: ì œëª© ê°€ì¤‘ì¹˜ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ, A1ì—ì„œ ì´ë¯¸ ë§¤ì¹­ ì™„ë£Œ)
            dense_weight: ì‹œë©˜í‹± ê°€ì¤‘ì¹˜ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ, A1ì—ì„œ ì´ë¯¸ ë§¤ì¹­ ì™„ë£Œ)

        Returns:
            ContentAnalysisResult: ì „ì²´ ë¶„ì„ ê²°ê³¼
        """
        if matching_types is None:
            matching_types = ["primary"]
        start_time = time.time()

        logger.info(f"A3 ë¶„ì„ ì‹œì‘: {contract_id} (type={contract_type}, matching_types={matching_types})")

        # ê²°ê³¼ ê°ì²´ ì´ˆê¸°í™”
        result = ContentAnalysisResult(
            contract_id=contract_id,
            contract_type=contract_type,
            analysis_timestamp=datetime.now()
        )

        # ì‚¬ìš©ì ê³„ì•½ì„œ ì¡°í•­ ì¶”ì¶œ
        articles = user_contract.get('articles', [])
        result.total_articles = len(articles)

        if not articles:
            logger.warning("  ë¶„ì„í•  ì¡°í•­ì´ ì—†ìŠµë‹ˆë‹¤")
            result.processing_time = time.time() - start_time
            return result

        # A1 ë§¤ì¹­ ê²°ê³¼ ë¡œë“œ (matching_typesì— ë”°ë¼ í•„í„°ë§)
        a1_matching_details = self._load_a1_matching_results(contract_id, matching_types)

        if not a1_matching_details:
            logger.warning("  A1 ë§¤ì¹­ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            result.processing_time = time.time() - start_time
            return result

        # ì¡°í•­ ë²ˆí˜¸ë³„ ë§¤ì¹­ ê²°ê³¼ ë§¤í•‘
        a1_results_by_article = {
            detail['user_article_no']: detail
            for detail in a1_matching_details
        }

        logger.info(f"  A1 ë§¤ì¹­ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ: {len(a1_results_by_article)}ê°œ ì¡°í•­")

        # ë³‘ë ¬ ì¡°í•­ ë¶„ì„
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        logger.info(f"ğŸš€ A3 ì¡°í•­ ë¶„ì„ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘: {len(articles)}ê°œ ì¡°í•­ (max_workers=4)")
        
        def process_single_article(article):
            """ë‹¨ì¼ ì¡°í•­ ë¶„ì„"""
            try:
                article_no = article.get('number')
                a1_result = a1_results_by_article.get(article_no)

                analysis = self.analyze_article(
                    article,
                    contract_type,
                    contract_id,
                    a1_matching_result=a1_result
                )
                logger.info("--------------------------------------------------------------------------------")
                return analysis
            except Exception as e:
                logger.error(f"  ì¡°í•­ ë¶„ì„ ì‹¤íŒ¨ (ì œ{article.get('number')}ì¡°): {e}")
                logger.info("--------------------------------------------------------------------------------")
                return None
        
        # ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=4) as executor:
            future_to_article = {
                executor.submit(process_single_article, article): article
                for article in articles
            }
            
            for future in as_completed(future_to_article):
                analysis = future.result()
                
                if analysis:
                    result.article_analysis.append(analysis)
                    
                    if analysis.matched:
                        result.analyzed_articles += 1
                    if analysis.is_special:
                        result.special_articles += 1
        
        logger.info(f"âœ¨ A3 ì¡°í•­ ë¶„ì„ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ")

        # ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡
        result.processing_time = time.time() - start_time

        logger.info(f"A3 ë¶„ì„ ì™„ë£Œ: {result.analyzed_articles}/{result.total_articles}ê°œ ì¡°í•­ ë¶„ì„ ({result.processing_time:.2f}ì´ˆ)")
        logger.info("================================================================================")
        logger.info("================================================================================")

        return result
    
    def analyze_article(
        self,
        user_article: Dict[str, Any],
        contract_type: str,
        contract_id: str = None,
        a1_matching_result: Dict[str, Any] = None
    ) -> ArticleAnalysis:
        """
        ë‹¨ì¼ ì¡°í•­ ë¶„ì„ (A1 ë§¤ì¹­ ê²°ê³¼ ì°¸ì¡°)

        Args:
            user_article: ì‚¬ìš©ì ê³„ì•½ì„œ ì¡°í•­
            contract_type: ê³„ì•½ ìœ í˜•
            contract_id: ê³„ì•½ì„œ ID (í† í° ë¡œê¹…ìš©)
            a1_matching_result: A1 ë…¸ë“œì˜ ë§¤ì¹­ ê²°ê³¼ (í•´ë‹¹ ì¡°í•­)

        Returns:
            ArticleAnalysis: ì¡°í•­ ë¶„ì„ ê²°ê³¼
        """
        article_no = user_article.get('number')
        article_title = user_article.get('title', '')

        logger.info(f"  ì¡°í•­ ë¶„ì„: ì œ{article_no}ì¡° ({article_title})")

        # ê¸°ë³¸ ë¶„ì„ ê°ì²´ ìƒì„±
        analysis = ArticleAnalysis(
            user_article_no=article_no,
            user_article_title=article_title,
            matched=False,
            similarity=0.0,
            analysis_timestamp=datetime.now()
        )

        try:
            # A1 ë§¤ì¹­ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë¶„ì„ ë¶ˆê°€
            if not a1_matching_result:
                logger.warning(f"    A1 ë§¤ì¹­ ê²°ê³¼ ì—†ìŒ")
                analysis.reasoning = "A1 ë§¤ì¹­ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                return analysis

            # A1 ë§¤ì¹­ ê²°ê³¼ ì°¸ì¡°
            analysis.matched = a1_matching_result.get('matched', False)

            # A1ì—ì„œëŠ” is_special í•„ë“œê°€ ì—†ìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©
            analysis.is_special = False

            # A1 ë§¤ì¹­ ì •ë³´
            matched_article_ids = a1_matching_result.get('matched_articles_global_ids', [])
            matched_details = a1_matching_result.get('matched_articles_details', [])

            # fallback: global_idê°€ ì—†ìœ¼ë©´ parent_id ì‚¬ìš©
            if not matched_article_ids:
                matched_article_ids = a1_matching_result.get('matched_articles', [])

            if not analysis.matched or not matched_article_ids:
                logger.info(f"    ë§¤ì¹­ ì‹¤íŒ¨: A1ì—ì„œ ë§¤ì¹­ë˜ì§€ ì•ŠìŒ")
                analysis.reasoning = "A1 ë§¤ì¹­ ê²€ì¦ í†µê³¼ ëª»í•¨"
                return analysis

            # A1 ìƒì„¸ ì •ë³´ì—ì„œ ì ìˆ˜ ë§¤í•‘ ìƒì„±
            score_map = {}
            for detail in matched_details:
                parent_id = detail.get('parent_id')
                global_id = detail.get('global_id')
                score_map[parent_id] = detail
                score_map[global_id] = detail

            # ë§¤ì¹­ëœ ì¡°í•­ ì •ë³´ êµ¬ì„± (A1 ê²°ê³¼ ê¸°ë°˜)
            # A1ì˜ matched_articles_global_idsëŠ” global_id ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["urn:std:provide:art:005"])
            # A3ëŠ” ìƒì„¸ ì •ë³´ê°€ í•„ìš”í•˜ë¯€ë¡œ í‘œì¤€ê³„ì•½ì„œ ì²­í¬ ë¡œë“œ
            analysis.matched_articles = []

            for std_article_id in matched_article_ids:
                # í•´ë‹¹ ì¡°ì˜ ì²­í¬ ë¡œë“œ (global_id ì§€ì›)
                chunks = self._load_standard_article_chunks(
                    std_article_id,
                    contract_type
                )

                if chunks:
                    # A1 ìƒì„¸ ì •ë³´ì—ì„œ ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°
                    detail_info = score_map.get(std_article_id, {})

                    # ì¡° ì •ë³´ ì¶”ê°€
                    article_info = {
                        'parent_id': detail_info.get('parent_id', std_article_id),
                        'global_id': std_article_id,  # global_id ì¶”ê°€
                        'title': chunks[0].get('title', ''),
                        'score': detail_info.get('combined_score', 0.0),  # A1ì˜ ì ìˆ˜ ì‚¬ìš©
                        'num_sub_items': detail_info.get('num_sub_items', 0),  # A1ì˜ í•˜ìœ„í•­ëª© ê°œìˆ˜
                        'matched_sub_items': detail_info.get('matched_sub_items', []),  # A1ì˜ ë§¤ì¹­ëœ í•˜ìœ„í•­ëª© ì¸ë±ìŠ¤
                        'matched_chunks': chunks
                    }
                    analysis.matched_articles.append(article_info)

            if not analysis.matched_articles:
                logger.warning(f"    ì²­í¬ ë¡œë“œ ì‹¤íŒ¨: ëª¨ë“  í‘œì¤€ ì¡°í•­")
                analysis.reasoning = "í‘œì¤€ê³„ì•½ì„œ ì¡°í•­ ë¡œë“œ ì‹¤íŒ¨"
                return analysis

            # ì²« ë²ˆì§¸ ì¡° ì •ë³´ (UI í‘œì‹œìš©)
            first_article = analysis.matched_articles[0]
            analysis.similarity = first_article.get('score', 0.0)
            analysis.std_article_id = first_article['parent_id']
            analysis.std_article_title = first_article.get('title', '')

            # A1ì˜ ìƒì„¸ ë§¤ì¹­ ì •ë³´ë¥¼ A3 ê²°ê³¼ì— í¬í•¨ (í”„ë¡ íŠ¸ì—”ë“œ í•˜ìœ„í•­ëª© ë“œëë‹¤ìš´ìš©)
            analysis.matched_articles_details = matched_details

            # ë§¤ì¹­ ì„±ê³µ
            logger.info(f"    ë§¤ì¹­ ì„±ê³µ (A1 ì°¸ì¡°): {analysis.std_article_id}")
            logger.info(f"    ë§¤ì¹­ëœ ì¡°í•­: {len(analysis.matched_articles)}ê°œ")

            # ì—¬ëŸ¬ ì¡°ê°€ ë§¤ì¹­ëœ ê²½ìš°
            if len(analysis.matched_articles) > 1:
                other_articles = [a['parent_id'] for a in analysis.matched_articles[1:]]
                logger.info(f"    ê¸°íƒ€ ë§¤ì¹­ ì¡°: {', '.join(other_articles)}")
                analysis.reasoning = f"í‘œì¤€ê³„ì•½ì„œ {analysis.std_article_id}ì™€ ë§¤ì¹­ë¨. ê¸°íƒ€ ë§¤ì¹­: {', '.join(other_articles)}"
            else:
                analysis.reasoning = f"í‘œì¤€ê³„ì•½ì„œ {analysis.std_article_id}ì™€ ë§¤ì¹­ë¨"

            # ContentComparatorë¡œ ë‚´ìš© ë¹„êµ
            # ìƒìœ„ 4ê°œ ë§¤ì¹­ ì¡°ì˜ ì²­í¬ ë¡œë“œ (ì´ë¯¸ ë¡œë“œë¨)
            top_matched_articles = analysis.matched_articles[:4]
            standard_chunks_list = [
                article['matched_chunks'] for article in top_matched_articles
                if article.get('matched_chunks')
            ]

            if len(analysis.matched_articles) > 4:
                logger.info(f"      ë§¤ì¹­ ì¡° {len(analysis.matched_articles)}ê°œ ì¤‘ ìƒìœ„ 4ê°œë§Œ LLM ë¹„êµ ìˆ˜í–‰")

            if not standard_chunks_list:
                logger.warning(f"      ëª¨ë“  í‘œì¤€ê³„ì•½ì„œ ì¡° ì²­í¬ ë¡œë“œ ì‹¤íŒ¨")
            else:
                # ì „ì²´ ì²­í¬ ë¡œë“œ (ì°¸ì¡° í•´ê²°ìš©)
                all_chunks = self.kb_loader.load_chunks(contract_type)
                
                # LLM ë¹„êµ ìˆ˜í–‰
                # A1ì—ì„œ ì´ë¯¸ ê´€ë ¨ ì¡°í•­ì„ ì„ íƒí–ˆìœ¼ë¯€ë¡œ
                # 1ê°œ: ì§ì ‘ ë¹„êµ
                # 2ê°œ ì´ìƒ: ëª¨ë“  ì¡°í•­ì„ ì¢…í•©í•˜ì—¬ ë¹„êµ
                comparison_result = self.content_comparator.compare_articles(
                    user_article,
                    standard_chunks_list,
                    contract_type,
                    all_chunks=all_chunks
                )

                # ì„ íƒëœ ì¡°í•­ ì •ë³´
                selected_article_ids = comparison_result.get('selected_articles', [])
                logger.info(f"      ë‚´ìš© ë¹„êµ ëŒ€ìƒ: {', '.join(selected_article_ids)}")

                # í† í° ì‚¬ìš©ëŸ‰ ë¡œê¹…
                if contract_id:
                    self._log_token_usage(
                        contract_id,
                        comparison_result.get('prompt_tokens', 0),
                        comparison_result.get('completion_tokens', 0),
                        comparison_result.get('total_tokens', 0),
                        extra_info={
                            'operation': 'content_comparison',
                            'user_article_no': article_no,
                            'num_candidates': len(analysis.matched_articles),
                            'selected_articles': selected_article_ids
                        }
                    )

                # ì œì•ˆ ìƒì„± (analysisê°€ ìˆìœ¼ë©´ í•­ìƒ ìƒì„±)
                analysis_text = comparison_result.get('analysis', '')
                if analysis_text:
                    if comparison_result.get('has_issues'):
                        # ë¬¸ì œê°€ ìˆëŠ” ê²½ìš°
                        missing_count = len(comparison_result.get('missing_items', []))
                        insufficient_count = len(comparison_result.get('insufficient_items', []))

                        # ì‹¬ê°ë„ ê²°ì •
                        # high: ëˆ„ë½ í•­ëª© 3ê°œ ì´ìƒ ë˜ëŠ” (ëˆ„ë½ + ë¶ˆì¶©ë¶„) 5ê°œ ì´ìƒ
                        # medium: ëˆ„ë½ í•­ëª© 1ê°œ ì´ìƒ ë˜ëŠ” ë¶ˆì¶©ë¶„ í•­ëª© 2ê°œ ì´ìƒ
                        # low: ê·¸ ì™¸
                        if missing_count >= 3 or (missing_count + insufficient_count) >= 5:
                            severity = 'high'
                        elif missing_count >= 2 or insufficient_count >= 2:
                            severity = 'medium'
                        else:
                            severity = 'low'

                        suggestion = {
                            'selected_standard_articles': selected_article_ids,
                            'issue_type': 'content',
                            'missing_items': comparison_result.get('missing_items', []),
                            'insufficient_items': comparison_result.get('insufficient_items', []),
                            'analysis': analysis_text,
                            'severity': severity
                        }
                        analysis.suggestions.append(suggestion)
                        logger.info(f"      ë‚´ìš© ë¶„ì„: ë¬¸ì œ ë°œê²¬ (ëˆ„ë½: {missing_count}ê°œ, ë¶ˆì¶©ë¶„: {insufficient_count}ê°œ, ì‹¬ê°ë„: {severity})")
                    else:
                        # ë¬¸ì œê°€ ì—†ëŠ” ê²½ìš° (ê¸ì •ì  ë¶„ì„)
                        suggestion = {
                            'selected_standard_articles': selected_article_ids,
                            'issue_type': 'content',
                            'missing_items': [],
                            'insufficient_items': [],
                            'analysis': analysis_text,
                            'severity': 'info'  # ì •ë³´ì„± (ë¬¸ì œ ì—†ìŒ)
                        }
                        analysis.suggestions.append(suggestion)
                        logger.info(f"      ë‚´ìš© ë¶„ì„: ë¬¸ì œ ì—†ìŒ (ê¸ì •ì  ë¶„ì„ í¬í•¨)")

        except Exception as e:
            logger.error(f"    ì¡°í•­ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            analysis.reasoning = f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

        return analysis

    def _load_standard_article_chunks(
        self,
        article_identifier: str,
        contract_type: str
    ) -> List[Dict[str, Any]]:
        """
        Load chunk records for a standard article identified by parent_id or global_id.

        Args:
            article_identifier: Either a parent_id (e.g. "ì¡°ë¬¸5") or global_id (e.g. "urn:std:provide:art:005").
            contract_type: Contract type key used by the knowledge base loader.

        Returns:
            List of chunk dictionaries that belong to the article.
        """
        logger.debug(f"  Loading standard article chunks: {article_identifier}")

        try:
            chunks = self.kb_loader.load_chunks(contract_type)
        except Exception as exc:
            logger.error(f"    Failed to load chunks for {contract_type}: {exc}")
            return []

        if not chunks:
            logger.warning(f"    Chunk data not available: {contract_type}")
            return []

        if article_identifier.startswith("urn:"):
            article_chunks = [
                chunk for chunk in chunks
                if chunk.get("global_id", "").startswith(article_identifier)
            ]
        else:
            article_chunks = [
                chunk for chunk in chunks
                if chunk.get("parent_id") == article_identifier
            ]

        article_chunks.sort(key=lambda x: x.get("order_index", 0))
        logger.debug(f"    Loaded {len(article_chunks)} chunk(s) for {article_identifier}")
        return article_chunks

    def _load_a1_matching_results(self, contract_id: str, matching_types: List[str] = None) -> List[Dict[str, Any]]:
        """
        A1 ë…¸ë“œì˜ ë§¤ì¹­ ê²°ê³¼ë¥¼ DBì—ì„œ ë¡œë“œ

        Args:
            contract_id: ê³„ì•½ì„œ ID
            matching_types: ì²˜ë¦¬í•  ë§¤ì¹­ ìœ í˜• (["primary"], ["recovered"], ["primary", "recovered"])
                           Noneì´ë©´ ["primary"] ì‚¬ìš©

        Returns:
            A1 ë§¤ì¹­ ê²°ê³¼ì˜ matching_details (ì¡°í•­ë³„ ë§¤ì¹­ ì •ë³´ ë¦¬ìŠ¤íŠ¸)
        """
        if matching_types is None:
            matching_types = ["primary"]
        
        try:
            from backend.shared.database import SessionLocal, ValidationResult

            db = SessionLocal()
            try:
                # ValidationResultì—ì„œ completeness_check í•„ë“œ ë¡œë“œ
                validation_result = db.query(ValidationResult).filter(
                    ValidationResult.contract_id == contract_id
                ).first()

                if not validation_result:
                    logger.warning(f"  ValidationResultë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {contract_id}")
                    return []

                completeness_check = validation_result.completeness_check
                if not completeness_check:
                    logger.warning(f"  A1 ì™„ì „ì„± ê²€ì¦ ê²°ê³¼ê°€ ì—†ìŒ: {contract_id}")
                    return []

                # matching_typesì— ë”°ë¼ ê²°ê³¼ ìˆ˜ì§‘
                all_matching_details = []
                if "primary" in matching_types:
                    all_matching_details.extend(completeness_check.get('matching_details', []))
                if "recovered" in matching_types:
                    all_matching_details.extend(completeness_check.get('recovered_matching_details', []))

                logger.debug(f"  A1 ë§¤ì¹­ ê²°ê³¼ ë¡œë“œ: {len(all_matching_details)}ê°œ ì¡°í•­ (types={matching_types})")

                return all_matching_details

            finally:
                db.close()

        except Exception as e:
            logger.error(f"  A1 ë§¤ì¹­ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []

    def _log_token_usage(
        self,
        contract_id: str,
        prompt_tokens: int,
        completion_tokens: int,
        total_tokens: int,
        extra_info: dict = None
    ):
        """
        í† í° ì‚¬ìš©ëŸ‰ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ë¡œê¹…

        Args:
            contract_id: ê³„ì•½ì„œ ID
            prompt_tokens: ì…ë ¥ í† í° ìˆ˜
            completion_tokens: ì¶œë ¥ í† í° ìˆ˜
            total_tokens: ì´ í† í° ìˆ˜
            extra_info: ì¶”ê°€ ì •ë³´ (operation, article_no ë“±)
        """
        try:
            from backend.shared.database import SessionLocal, TokenUsage
            from datetime import datetime

            db = SessionLocal()
            try:
                token_record = TokenUsage(
                    contract_id=contract_id,
                    component="consistency_agent",
                    api_type="chat_completion",
                    model=self.content_comparator.model,
                    prompt_tokens=prompt_tokens,
                    completion_tokens=completion_tokens,
                    total_tokens=total_tokens,
                    created_at=datetime.utcnow(),
                    extra_info=extra_info
                )
                db.add(token_record)
                db.commit()
                logger.debug(f"      í† í° ì‚¬ìš©ëŸ‰ ë¡œê¹… ì™„ë£Œ: {total_tokens} tokens")
            finally:
                db.close()

        except Exception as e:
            logger.error(f"      í† í° ì‚¬ìš©ëŸ‰ ë¡œê¹… ì‹¤íŒ¨: {e}")
