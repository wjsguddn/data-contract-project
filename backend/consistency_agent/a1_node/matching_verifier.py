"""
MatchingVerifier - LLM ê¸°ë°˜ ì¡°í•­ ë§¤ì¹­ ê²€ì¦

ê²€ìƒ‰ ì—”ì§„ìœ¼ë¡œ ì¶”ì¶œëœ í›„ë³´ ì¡°í•­ë“¤ ì¤‘ ì‹¤ì œë¡œ ë§¤ì¹­ë˜ëŠ” ì¡°í•­ì„ LLMìœ¼ë¡œ ê²€ì¦
"""

import logging
from typing import Dict, Any, List, Optional
from openai import AzureOpenAI

logger = logging.getLogger(__name__)


class MatchingVerifier:
    """
    LLM ê¸°ë°˜ ë§¤ì¹­ ê²€ì¦ê¸°

    ì£¼ìš” ê¸°ëŠ¥:
    1. í›„ë³´ ì¡°í•­ë“¤ ì¤‘ ì‹¤ì œ ê´€ë ¨ìˆëŠ” ì¡°í•­ ì„ íƒ (LLM)
    2. ì„ íƒëœ ì¡°í•­ë“¤ì— ëŒ€í•œ ë§¤ì¹­ ì—¬ë¶€ ìµœì¢… ê²€ì¦ (LLM)
    """

    def __init__(self, azure_client: AzureOpenAI, model: str = "gpt-4o", knowledge_base_loader=None):
        """
        Args:
            azure_client: Azure OpenAI í´ë¼ì´ì–¸íŠ¸
            model: ì‚¬ìš©í•  ëª¨ë¸ëª… (ê¸°ë³¸: gpt-4o)
            knowledge_base_loader: KnowledgeBaseLoader ì¸ìŠ¤í„´ìŠ¤ (ì°¸ì¡° ë¡œë“œìš©)
        """
        self.azure_client = azure_client
        self.model = model
        self.kb_loader = knowledge_base_loader
        self.all_chunks = []  # ì°¸ì¡° ë¡œë“œìš© ì „ì²´ ì²­í¬ ìºì‹œ

        logger.info(f"MatchingVerifier ì´ˆê¸°í™” ì™„ë£Œ (model={model})")

    def verify_matching(
        self,
        user_article: Dict[str, Any],
        candidate_articles: List[Dict[str, Any]],
        contract_type: str,
        top_k: int = 5
    ) -> Dict[str, Any]:
        """
        í›„ë³´ ì¡°í•­ë“¤ì— ëŒ€í•œ ë§¤ì¹­ ê²€ì¦

        Args:
            user_article: ì‚¬ìš©ì ì¡°í•­ (content ë°°ì—´ í¬í•¨)
            candidate_articles: í›„ë³´ í‘œì¤€ê³„ì•½ì„œ ì¡°í•­ ëª©ë¡ (ì¡° ë‹¨ìœ„ ì§‘ê³„ ê²°ê³¼)
                [
                    {
                        'parent_id': str,
                        'title': str,
                        'score': float,
                        'matched_sub_items': List[int],
                        'num_sub_items': int,
                        'matched_chunks': List[Dict]
                    },
                    ...
                ]
            contract_type: ê³„ì•½ ìœ í˜•
            top_k: ìµœì¢… ì„ íƒí•  ì¡°í•­ ê°œìˆ˜ (ê¸°ë³¸: 5)

        Returns:
            {
                "matched": bool,
                "selected_articles": List[str],  # ë§¤ì¹­ëœ ì¡°í•­ IDë“¤
                "verification_details": List[Dict],  # ê° ì¡°í•­ë³„ ê²€ì¦ ìƒì„¸
                "prompt_tokens": int,
                "completion_tokens": int,
                "total_tokens": int
            }
        """
        if not candidate_articles:
            logger.warning(f"  í›„ë³´ ì¡°í•­ì´ ì—†ìŠµë‹ˆë‹¤")
            return {
                "matched": False,
                "selected_articles": [],
                "verification_details": [],
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0
            }

        # Top-K í›„ë³´ ì„ ì •
        top_candidates = candidate_articles[:top_k]

        logger.info(f"  ë§¤ì¹­ ê²€ì¦ ì‹œì‘: í›„ë³´ {len(top_candidates)}ê°œ ì¡°í•­")

        # 1ë‹¨ê³„: ê´€ë ¨ ì¡°í•­ ì„ íƒ
        selection_result = self._select_relevant_articles(
            user_article,
            top_candidates,
            contract_type
        )

        selected_article_ids = selection_result.get('selected_articles', [])

        if not selected_article_ids:
            logger.warning(f"  ë§¤ì¹­ ê²€ì¦ ì‹¤íŒ¨: LLMì´ ê´€ë ¨ ì¡°í•­ì„ ì„ íƒí•˜ì§€ ëª»í•¨")
            return {
                "matched": False,
                "selected_articles": [],
                "verification_details": [],
                "prompt_tokens": selection_result.get('prompt_tokens', 0),
                "completion_tokens": selection_result.get('completion_tokens', 0),
                "total_tokens": selection_result.get('prompt_tokens', 0) + selection_result.get('completion_tokens', 0)
            }

        logger.info(f"  ë§¤ì¹­ ê²€ì¦ ì™„ë£Œ: {len(selected_article_ids)}ê°œ ì¡°í•­ ì„ íƒ")

        return {
            "matched": True,
            "selected_articles": selected_article_ids,
            "verification_details": [],  # TODO: í•„ìš”ì‹œ ìƒì„¸ ì •ë³´ ì¶”ê°€
            "prompt_tokens": selection_result.get('prompt_tokens', 0),
            "completion_tokens": selection_result.get('completion_tokens', 0),
            "total_tokens": selection_result.get('prompt_tokens', 0) + selection_result.get('completion_tokens', 0)
        }

    def _select_relevant_articles(
        self,
        user_article: Dict[str, Any],
        candidate_articles: List[Dict[str, Any]],
        contract_type: str
    ) -> Dict[str, Any]:
        """
        ê´€ë ¨ í‘œì¤€ ì¡°í•­ ì„ íƒ (LLM í™œìš©)

        Args:
            user_article: ì‚¬ìš©ì ì¡°í•­
            candidate_articles: í›„ë³´ í‘œì¤€ê³„ì•½ì„œ ì¡°í•­ë“¤ (ì¡° ë‹¨ìœ„ ì§‘ê³„ ê²°ê³¼)
            contract_type: ê³„ì•½ ìœ í˜•

        Returns:
            {
                "selected_articles": List[str],  # ì„ íƒëœ ì¡°í•­ IDë“¤
                "prompt_tokens": int,
                "completion_tokens": int
            }
        """
        # ì‚¬ìš©ì ì¡°í•­ í¬ë§·íŒ…
        user_text = self._format_user_article(user_article)

        # í›„ë³´ ì¡°í•­ë“¤ í¬ë§·íŒ…
        candidates_text = ""
        for candidate in candidate_articles:
            parent_id = candidate['parent_id']
            title = candidate['title']
            score = candidate['score']
            num_sub_items = candidate['num_sub_items']

            # í•´ë‹¹ ì¡°ì˜ ì²­í¬ë“¤ í¬ë§·íŒ… (ì‹ ê·œ êµ¬ì¡°: text_norm + commentary_summary + references)
            chunks = candidate.get('matched_chunks', [])
            chunk_lines = []
            for chunk_data in chunks:
                chunk = chunk_data.get('chunk', {})
                chunk_id = chunk.get('id', '')
                text_norm = chunk.get('text_norm', '').strip()
                commentary_summary = chunk.get('commentary_summary', '').strip()
                references = chunk.get('references', [])
                
                if not chunk_id or not text_norm:
                    continue
                
                # ê¸°ë³¸ í…ìŠ¤íŠ¸
                chunk_lines.append(f"  {chunk_id}: {text_norm}")
                
                # references ì²˜ë¦¬
                if references:
                    logger.info(f"      ì²­í¬ {chunk_id}ì— references ë°œê²¬: {len(references)}ê°œ")
                    has_exhibit_ref = any(':ex:' in ref for ref in references)
                    
                    if has_exhibit_ref:
                        logger.info(f"      ë³„ì§€ ì°¸ì¡° ê°ì§€: {[r for r in references if ':ex:' in r]}")
                        # ë³„ì§€ ì°¸ì¡°: text_llm ë¡œë“œ
                        exhibit_contents = self._load_referenced_exhibits(references)
                        if exhibit_contents:
                            chunk_lines.append("    [ì°¸ì¡° ë³„ì§€]")
                            for ref_id, ref_content in exhibit_contents.items():
                                chunk_lines.append(f"      {ref_id}: {ref_content}")
                        else:
                            logger.warning(f"      ë³„ì§€ ì°¸ì¡° ë¡œë“œ ì‹¤íŒ¨")
                    else:
                        logger.info(f"      ì¡°í•­ ì°¸ì¡° ê°ì§€: {references}")
                        # ì¡°í•­ ì°¸ì¡°: text_norm + commentary_summary ë¡œë“œ
                        article_contents = self._load_referenced_articles(references)
                        if article_contents:
                            chunk_lines.append("    [ì°¸ì¡° ì¡°í•­]")
                            for ref_id, ref_content in article_contents.items():
                                chunk_lines.append(f"      {ref_id}: {ref_content}")
                        else:
                            logger.warning(f"      ì¡°í•­ ì°¸ì¡° ë¡œë“œ ì‹¤íŒ¨")
                
                # commentary_summary (ë³„ì§€ ì°¸ì¡°ê°€ ì—†ëŠ” ê²½ìš°ë§Œ)
                if commentary_summary and not any(':ex:' in ref for ref in references):
                    chunk_lines.append(f"    [í•´ì„¤] {commentary_summary}")

            candidates_text += f"{parent_id} ({title}) [ìœ ì‚¬ë„: {score:.3f}, ë§¤ì¹­ í•˜ìœ„í•­ëª©: {num_sub_items}ê°œ]\n"
            candidates_text += "\n".join(chunk_lines)
            candidates_text += "\n\n---\n\n"

        # ì„ íƒ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_selection_prompt(
            user_article_no=user_article.get('number'),
            user_article_title=user_article.get('title', ''),
            user_text=user_text,
            candidates_text=candidates_text,
            contract_type=contract_type
        )

        try:
            response = self.azure_client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": """ë‹¹ì‹ ì€ ë°ì´í„° ê³„ì•½ì„œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 

ì—­í• : ì‚¬ìš©ì ê³„ì•½ì„œ ì¡°í•­ê³¼ í‘œì¤€ê³„ì•½ì„œ ì¡°í•­ ê°„ì˜ ê´€ë ¨ì„±ì„ íŒë‹¨í•©ë‹ˆë‹¤.

ì¤‘ìš”: 
- ë‚´ìš©ì˜ ì¶©ì‹¤ë„ë‚˜ ì™„ì„±ë„ëŠ” í‰ê°€í•˜ì§€ ë§ˆì„¸ìš” (ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ìˆ˜í–‰)
- ë‹¨ìˆœíˆ "ê°™ì€ ì£¼ì œë¥¼ ë‹¤ë£¨ëŠ”ê°€?"ë§Œ íŒë‹¨í•˜ì„¸ìš”
- ì˜ì‹¬ìŠ¤ëŸ¬ìš°ë©´ ê´€ë ¨ìˆìŒìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš” (ë³´ìˆ˜ì  ì ‘ê·¼)
- ëª…ë°±íˆ ë‹¤ë¥¸ ì£¼ì œë§Œ ì œì™¸í•˜ì„¸ìš”"""
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.2,
                max_tokens=500
            )

            selection_text = response.choices[0].message.content.strip()
            usage = response.usage

            # ì„ íƒëœ ì¡°í•­ ID íŒŒì‹±
            selected_ids = self._parse_selection_response(selection_text, candidate_articles)

            logger.info(f"    LLM ì¡°í•­ ì„ íƒ ì™„ë£Œ: {len(selected_ids)}ê°œ (í† í°: {usage.total_tokens})")

            return {
                "selected_articles": selected_ids,
                "prompt_tokens": usage.prompt_tokens,
                "completion_tokens": usage.completion_tokens
            }

        except Exception as e:
            logger.error(f"    ì¡°í•­ ì„ íƒ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ëª¨ë“  í›„ë³´ ë°˜í™˜
            all_ids = [candidate['parent_id'] for candidate in candidate_articles]
            return {
                "selected_articles": all_ids,
                "prompt_tokens": 0,
                "completion_tokens": 0
            }

    def _build_selection_prompt(
        self,
        user_article_no: int,
        user_article_title: str,
        user_text: str,
        candidates_text: str,
        contract_type: str
    ) -> str:
        """
        ì¡°í•­ ì„ íƒ í”„ë¡¬í”„íŠ¸ ìƒì„±

        Args:
            user_article_no: ì‚¬ìš©ì ì¡°í•­ ë²ˆí˜¸
            user_article_title: ì‚¬ìš©ì ì¡°í•­ ì œëª©
            user_text: í¬ë§·íŒ…ëœ ì‚¬ìš©ì ì¡°í•­
            candidates_text: í¬ë§·íŒ…ëœ í›„ë³´ ì¡°í•­ë“¤
            contract_type: ê³„ì•½ ìœ í˜•

        Returns:
            í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
        """
        contract_type_names = {
            "provide": "ë°ì´í„° ì œê³µ ê³„ì•½",
            "create": "ë°ì´í„° ìƒì„± ê³„ì•½",
            "process": "ë°ì´í„° ê°€ê³µ ê³„ì•½",
            "brokerage_provider": "ë°ì´í„° ì¤‘ê°œ ê³„ì•½ (ì œê³µììš©)",
            "brokerage_user": "ë°ì´í„° ì¤‘ê°œ ê³„ì•½ (ì´ìš©ììš©)"
        }

        contract_name = contract_type_names.get(contract_type, contract_type)

        prompt = f"""# ê´€ë ¨ í‘œì¤€ ì¡°í•­ ì„ íƒ

## ê³„ì•½ ìœ í˜•
{contract_name}

## ì‚¬ìš©ì ê³„ì•½ì„œ ì¡°í•­
ì œ{user_article_no}ì¡° ({user_article_title})
{user_text}

## í›„ë³´ í‘œì¤€ê³„ì•½ì„œ ì¡°í•­ë“¤
ì•„ë˜ ì¡°í•­ë“¤ì€ ì‚¬ìš©ì ì¡°í•­ê³¼ ì—°ê´€ë˜ì–´ ìˆì„ ê°€ëŠ¥ì„±ì´ ìˆëŠ” í‘œì¤€ê³„ì•½ì„œ ì¡°í•­ë“¤ì…ë‹ˆë‹¤.

{candidates_text}

---

**ê³¼ì œ**: ìœ„ì˜ í›„ë³´ ì¡°í•­ë“¤ ì¤‘ì—ì„œ ì‚¬ìš©ì ê³„ì•½ì„œ ì¡°í•­(ì œ{user_article_no}ì¡°)ê³¼ **ì‹¤ì œë¡œ ê´€ë ¨ìˆëŠ”** í‘œì¤€ê³„ì•½ì„œ ì¡°í•­ë“¤ì„ **ëª¨ë‘** ì„ íƒí•˜ì„¸ìš”.

**ë§¤ì¹­ íŒë‹¨ ê¸°ì¤€** (ê´€ë ¨ì„±ë§Œ íŒë‹¨, ë‚´ìš© ì¶©ì‹¤ë„ëŠ” í‰ê°€í•˜ì§€ ì•ŠìŒ):

1. **ê°™ì€ ì£¼ì œë¥¼ ë‹¤ë£¨ëŠ”ê°€?**
   - ì‚¬ìš©ì ì¡°í•­ê³¼ í‘œì¤€ ì¡°í•­ì´ ë™ì¼í•˜ê±°ë‚˜ ìœ ì‚¬í•œ ë²•ì  ì‚¬í•­ì„ ê·œìœ¨í•˜ëŠ”ê°€?
   - ë¶€ë¶„ì ìœ¼ë¡œë¼ë„ ê²¹ì¹˜ëŠ” ë‚´ìš©ì´ ìˆìœ¼ë©´ ê´€ë ¨ìˆìŒìœ¼ë¡œ íŒë‹¨

2. **ì—¬ëŸ¬ ì¡°í•­ ë§¤ì¹­ ê°€ëŠ¥**
   - ì‚¬ìš©ì ì¡°í•­ í•˜ë‚˜ê°€ í‘œì¤€ê³„ì•½ì„œì˜ ì—¬ëŸ¬ ì¡°í•­ì— ê±¸ì³ ìˆì„ ìˆ˜ ìˆìŒ
   - ê´€ë ¨ìˆëŠ” ì¡°í•­ì€ ëª¨ë‘ ì„ íƒ (1ê°œ ì´ìƒ)

3. **ì œì™¸ ê¸°ì¤€**
   - í‚¤ì›Œë“œë§Œ ìœ ì‚¬í•˜ê³  ë²•ì  ë§¥ë½ì´ ì™„ì „íˆ ë‹¤ë¥¸ ê²½ìš°
   - ì˜ˆ: "ë°ì´í„°"ë¼ëŠ” ë‹¨ì–´ë§Œ ê³µí†µì´ê³  ì‹¤ì œ ê·œìœ¨ ëŒ€ìƒì´ ë‹¤ë¦„

**ì¡°í•­ ìœ í˜•ë³„ íŒë‹¨ ê°€ì´ë“œ**:

**[ìš©ì–´ ì •ì˜ ì¡°í•­]**
- ìš©ì–´ì˜ ì˜ë¯¸/ë²”ìœ„ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì •ë¦½í•˜ëŠ” ë‚´ìš©ì´ ìˆì–´ì•¼ í•¨
- í‘œí˜„ í˜•ì‹: "~ë€", "~ëŠ”", "~ë¥¼ ì˜ë¯¸í•œë‹¤", "~ì„ ë§í•œë‹¤" ë“± ë‹¤ì–‘
- ê³„ì•½ ì „ì²´ì—ì„œ ë°˜ë³µ ì°¸ì¡°ë˜ëŠ” í•µì‹¬ ê°œë…ì˜ ë²”ìœ„ë¥¼ ëª…ì‹œ
- ì œì™¸: ê³„ì•½ ëª©ì , ë°°ê²½, ì²´ê²° ê²½ìœ„, ë‹¹ì‚¬ì ì†Œê°œ ë“±

**[ê¶Œë¦¬/ì˜ë¬´ ì¡°í•­]**
- ë‹¹ì‚¬ìì˜ ê¶Œë¦¬ë‚˜ ì˜ë¬´ë¥¼ ê·œì •í•˜ëŠ” ì¡°í•­
- "~í•  ìˆ˜ ìˆë‹¤", "~í•´ì•¼ í•œë‹¤", "~í•˜ì—¬ì„œëŠ” ì•„ë‹ˆ ëœë‹¤" ë“±
- ë¶€ë¶„ì  ê¶Œë¦¬/ì˜ë¬´ í¬í•¨ë„ ê´€ë ¨ìˆìŒìœ¼ë¡œ íŒë‹¨

**[ì ˆì°¨/ë°©ë²• ì¡°í•­]**
- íŠ¹ì • í–‰ìœ„ì˜ ì ˆì°¨, ë°©ë²•, ê¸°í•œì„ ê·œì •
- ë™ì¼í•œ ì ˆì°¨ë¥¼ ë‹¤ë£¨ë©´ ê´€ë ¨ìˆìŒ (ì„¸ë¶€ì‚¬í•­ ì°¨ì´ëŠ” ë¬´ì‹œ)

**[ì±…ì„/ì œì¬ ì¡°í•­]**
- ìœ„ë°˜ ì‹œ ì±…ì„, ì†í•´ë°°ìƒ, ê³„ì•½ í•´ì§€ ë“±
- ë™ì¼í•œ ìœ„ë°˜ ì‚¬í•­ì— ëŒ€í•œ ì œì¬ë©´ ê´€ë ¨ìˆìŒ

**[ê¸°íƒ€ ì¡°í•­]**
- ê³„ì•½ ê¸°ê°„, ë¹„ë°€ìœ ì§€, ë¶„ìŸ í•´ê²° ë“±
- ì£¼ì œê°€ ëª…í™•íˆ ì¼ì¹˜í•˜ë©´ ê´€ë ¨ìˆìŒ

**íŒë‹¨ ì›ì¹™**:
- ë‚´ìš©ì´ 100% ì¼ì¹˜í•  í•„ìš” ì—†ìŒ (ì¶©ì‹¤ë„ëŠ” A3ì—ì„œ í‰ê°€)
- ë¶€ë¶„ì ìœ¼ë¡œë¼ë„ ê´€ë ¨ìˆìœ¼ë©´ ì„ íƒ
- ì˜ì‹¬ìŠ¤ëŸ¬ìš°ë©´ ì„ íƒ (False Negative ë°©ì§€)
- ëª…ë°±íˆ ë‹¤ë¥¸ ì£¼ì œë§Œ ì œì™¸

**ì‘ë‹µ í˜•ì‹** (ì¡°í•­ IDë§Œ ë‚˜ì—´):
ì„ íƒëœ ì¡°í•­: ì œ1ì¡°, ì œ3ì¡°, ì œ5ì¡°

ë˜ëŠ” ê´€ë ¨ ì¡°í•­ì´ ì—†ëŠ” ê²½ìš°:
ì„ íƒëœ ì¡°í•­: ì—†ìŒ
"""

        return prompt

    def _parse_selection_response(
        self,
        response_text: str,
        candidate_articles: List[Dict[str, Any]]
    ) -> List[str]:
        """
        ì¡°í•­ ì„ íƒ ì‘ë‹µ íŒŒì‹±

        Args:
            response_text: LLM ì‘ë‹µ í…ìŠ¤íŠ¸
            candidate_articles: í›„ë³´ ì¡°í•­ ëª©ë¡

        Returns:
            ì„ íƒëœ ì¡°í•­ ID ë¦¬ìŠ¤íŠ¸
        """
        import re

        # ê°€ëŠ¥í•œ ëª¨ë“  ì¡°í•­ ID ì¶”ì¶œ
        available_ids = set()
        for candidate in candidate_articles:
            available_ids.add(candidate['parent_id'])

        # ì‘ë‹µì—ì„œ ì¡°í•­ ID íŒ¨í„´ ì°¾ê¸° (ì œNì¡°)
        pattern = r'ì œ\d+ì¡°'
        found_ids = re.findall(pattern, response_text)

        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” IDë§Œ í•„í„°ë§
        selected_ids = [id for id in found_ids if id in available_ids]

        # ì¤‘ë³µ ì œê±° ë° ìˆœì„œ ìœ ì§€
        seen = set()
        result = []
        for id in selected_ids:
            if id not in seen:
                seen.add(id)
                result.append(id)

        # ì•„ë¬´ê²ƒë„ ì„ íƒë˜ì§€ ì•Šì•˜ìœ¼ë©´ ëª¨ë“  í›„ë³´ ë°˜í™˜
        if not result:
            logger.warning(f"    ì‘ë‹µì—ì„œ ì¡°í•­ IDë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŒ, ëª¨ë“  í›„ë³´ ë°˜í™˜")
            result = list(available_ids)

        return result

    def _format_user_article(self, user_article: Dict[str, Any]) -> str:
        """
        ì‚¬ìš©ì ì¡°í•­ í¬ë§·íŒ…

        text
        content[0]
        content[1]
        ...

        Args:
            user_article: ì‚¬ìš©ì ì¡°í•­

        Returns:
            í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸
        """
        lines = []

        # text (ì¡° ë³¸ë¬¸)
        text = user_article.get('text', '').strip()
        if text:
            lines.append(text)

        # content ë°°ì—´ (í•˜ìœ„í•­ëª©ë“¤)
        content_items = user_article.get('content', [])
        for item in content_items:
            if item.strip():
                lines.append(item.strip())

        return "\n".join(lines)

    def verify_missing_article_forward(
        self,
        standard_article: Dict[str, Any],
        user_candidates: List[Dict[str, Any]],
        contract_type: str
    ) -> Dict[str, Any]:
        """
        ëˆ„ë½ ì¡°ë¬¸ ì—­ë°©í–¥ ê²€ì¦ (í‘œì¤€ â†’ ì‚¬ìš©ì)
        
        ëˆ„ë½ëœ ê²ƒìœ¼ë¡œ ì‹ë³„ëœ í‘œì¤€ ì¡°ë¬¸ì´ ì‹¤ì œë¡œ ì‚¬ìš©ì ê³„ì•½ì„œì— ì—†ëŠ”ì§€ ì¬í™•ì¸
        
        Args:
            standard_article: ëˆ„ë½ëœ í‘œì¤€ ì¡°ë¬¸
                {
                    'parent_id': str,
                    'title': str,
                    'chunks': List[Dict]
                }
            user_candidates: ìœ ì‚¬í•œ ì‚¬ìš©ì ì¡°ë¬¸ í›„ë³´ë“¤
                [
                    {
                        'user_article': Dict,
                        'similarity': float,
                        'matched_chunks': List[Dict]
                    },
                    ...
                ]
            contract_type: ê³„ì•½ ìœ í˜•
        
        Returns:
            {
                "is_truly_missing": bool,  # ì‹¤ì œë¡œ ëˆ„ë½ë˜ì—ˆëŠ”ì§€
                "confidence": float,  # ì‹ ë¢°ë„
                "matched_user_article": Dict or None,  # ë§¤ì¹­ëœ ì‚¬ìš©ì ì¡°ë¬¸ (ìˆë‹¤ë©´)
                "reasoning": str,  # íŒë‹¨ ê·¼ê±°
                "recommendation": str,  # ê¶Œê³ ì‚¬í•­
                "evidence": str,  # ìƒì„¸ ì¦ê±°
                "risk_assessment": str,  # ìœ„í—˜ë„ í‰ê°€
                "candidates_analysis": List[Dict],  # í›„ë³´ë³„ ë¶„ì„
                "prompt_tokens": int,
                "completion_tokens": int
            }
        """
        parent_id = standard_article.get('parent_id')
        title = standard_article.get('title', '')
        
        logger.info(f"  ëˆ„ë½ ì¡°ë¬¸ ì¬ê²€ì¦: {parent_id} ({title})")
        
        if not user_candidates:
            logger.warning(f"    í›„ë³´ ì¡°ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤ - LLMìœ¼ë¡œ ìƒì„¸ ë¶„ì„ ìƒì„±")
            # í›„ë³´ê°€ ì—†ì–´ë„ LLMìœ¼ë¡œ ìƒì„¸í•œ ëˆ„ë½ ë¶„ì„ ìƒì„±
            return self._generate_missing_analysis_without_candidates(
                standard_article,
                contract_type
            )
        
        # í‘œì¤€ ì¡°ë¬¸ í¬ë§·íŒ…
        standard_text = self._format_standard_article(standard_article)
        
        # í›„ë³´ ì¡°ë¬¸ë“¤ í¬ë§·íŒ…
        candidates_text = ""
        for i, candidate in enumerate(user_candidates, 1):
            user_article = candidate['user_article']
            similarity = candidate['similarity']
            user_no = user_article.get('number')
            user_title = user_article.get('title', '')
            user_text = self._format_user_article(user_article)
            
            candidates_text += f"**í›„ë³´ {i}: ì œ{user_no}ì¡° ({user_title})** (ìœ ì‚¬ë„: {similarity:.2f})\n"
            candidates_text += user_text
            candidates_text += "\n\n"
        
        # LLM í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_forward_verification_prompt(
            parent_id=parent_id,
            title=title,
            standard_text=standard_text,
            candidates_text=candidates_text,
            contract_type=contract_type
        )
        
        try:
            response = self.azure_client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ ê³„ì•½ì„œ ì¡°í•­ì„ ì •í™•í•˜ê²Œ ë¹„êµ ë¶„ì„í•˜ëŠ” ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.1,
                max_tokens=1500,
                response_format={"type": "json_object"}
            )
            
            response_text = response.choices[0].message.content.strip()
            usage = response.usage
            
            # ì‘ë‹µ íŒŒì‹±
            result = self._parse_forward_verification_response(
                response_text,
                user_candidates,
                standard_article
            )
            
            result['prompt_tokens'] = usage.prompt_tokens
            result['completion_tokens'] = usage.completion_tokens
            
            logger.info(f"    ì¬ê²€ì¦ ì™„ë£Œ: ëˆ„ë½={result['is_truly_missing']}, "
                       f"ì‹ ë¢°ë„={result['confidence']:.2f} (í† í°: {usage.total_tokens})")
            
            return result
            
        except Exception as e:
            logger.error(f"    ì¬ê²€ì¦ ì‹¤íŒ¨: {e}")
            return {
                "is_truly_missing": True,  # ì‹¤íŒ¨ ì‹œ ëˆ„ë½ìœ¼ë¡œ ê°„ì£¼
                "confidence": 0.5,
                "matched_user_article": None,
                "reasoning": f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "recommendation": f"'{title}' ì¡°í•­ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                "evidence": "LLM ê²€ì¦ ì‹¤íŒ¨",
                "risk_assessment": "ê²€ì¦ ì‹¤íŒ¨ë¡œ ì¸í•´ ì •í™•í•œ í‰ê°€ ë¶ˆê°€",
                "candidates_analysis": [],
                "prompt_tokens": 0,
                "completion_tokens": 0
            }
    
    def _format_standard_article(self, standard_article: Dict[str, Any]) -> str:
        """
        í‘œì¤€ ì¡°ë¬¸ í¬ë§·íŒ…
        
        Args:
            standard_article: í‘œì¤€ ì¡°ë¬¸
                {
                    'parent_id': str,
                    'title': str,
                    'chunks': List[Dict]
                }
        
        Returns:
            í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸
        """
        chunks = standard_article.get('chunks', [])
        
        lines = []
        for chunk in chunks:
            text = chunk.get('text_raw', '').strip()
            if text:
                lines.append(text)
        
        return "\n".join(lines)
    
    def _build_forward_verification_prompt(
        self,
        parent_id: str,
        title: str,
        standard_text: str,
        candidates_text: str,
        contract_type: str
    ) -> str:
        """
        ì—­ë°©í–¥ ê²€ì¦ í”„ë¡¬í”„íŠ¸ ìƒì„± (A1 ë¸Œëœì¹˜ ìŠ¤íƒ€ì¼)
        
        Args:
            parent_id: í‘œì¤€ ì¡°ë¬¸ ID
            title: í‘œì¤€ ì¡°ë¬¸ ì œëª©
            standard_text: í‘œì¤€ ì¡°ë¬¸ ë‚´ìš©
            candidates_text: í›„ë³´ ì‚¬ìš©ì ì¡°ë¬¸ë“¤
            contract_type: ê³„ì•½ ìœ í˜•
        
        Returns:
            í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
        """
        contract_type_names = {
            "provide": "ë°ì´í„° ì œê³µ ê³„ì•½",
            "create": "ë°ì´í„° ìƒì„± ê³„ì•½",
            "process": "ë°ì´í„° ê°€ê³µ ê³„ì•½",
            "brokerage_provider": "ë°ì´í„° ì¤‘ê°œ ê³„ì•½ (ì œê³µììš©)",
            "brokerage_user": "ë°ì´í„° ì¤‘ê°œ ê³„ì•½ (ì´ìš©ììš©)"
        }
        
        contract_name = contract_type_names.get(contract_type, contract_type)
        
        prompt = f"""ë‹¹ì‹ ì€ ê³„ì•½ì„œ ì¡°í•­ ë¶„ì„ ë° ë¦¬ìŠ¤í¬ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ë¶„ì„ ëŒ€ìƒ**
í‘œì¤€ ê³„ì•½ì„œì˜ "{parent_id} ({title})" ì¡°í•­ì´ ì‚¬ìš©ì ê³„ì•½ì„œì—ì„œ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.
í•´ë‹¹ ì¡°í•­ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ì‚¬ìš©ì ê³„ì•½ì„œ ì¡°í•­ Top-3ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.
ê° í›„ë³´ê°€ í‘œì¤€ ì¡°í•­ì˜ ë‚´ìš©ì„ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”.

**í‘œì¤€ ì¡°í•­ ({parent_id}):**
{standard_text}

**ì‚¬ìš©ì ê³„ì•½ì„œ í›„ë³´ ì¡°í•­ (Top-3):**
{candidates_text}

---

### ğŸ“‹ **íŒë‹¨ ì§€ì¹¨**
1. **ë¶€ë¶„ ì¼ì¹˜(í‘œí˜„ ì°¨ì´)**: í•µì‹¬ ë‚´ìš©ì€ ê°™ìœ¼ë‚˜ í‘œí˜„Â·ì¡°ê±´Â·ì ˆì°¨ê°€ ë‹¤ë¥´ê±°ë‚˜ í‘œí˜„ì´ ë‹¤ë¦„  
   (ì˜ˆ: 'ì œê³µí•œë‹¤' vs 'ì œê³µí•  ìˆ˜ ìˆë‹¤', 'ì‚¬ì „ ë™ì˜ ë°›ìœ¼ë©´ ë™ì˜' ì¤‘ í•˜ë‚˜ë§Œ í¬í•¨)
2. **ë¬´ê´€**: ë‚´ìš©ì ìœ¼ë¡œ ê´€ë ¨ì—†ìŒ
3. ë°˜ë“œì‹œ Top-3 í›„ë³´ ëª¨ë‘ì— ëŒ€í•´ íŒë‹¨í•˜ê³ , í‘œì¤€ì˜ í•µì‹¬ìš”ì†Œ ì¤‘ ì–´ë–¤ ë¶€ë¶„ì´ í¬í•¨/ëˆ„ë½ë˜ì—ˆëŠ”ì§€,  
   ê·¸ë¡œ ì¸í•œ ì ì¬ì  ë¦¬ìŠ¤í¬Â·ë²•ì Â·ìš´ì˜ìƒ ë¬¸ì œë¥¼ ëª…í™•íˆ ì„œìˆ í•  ê²ƒ
4. confidence: 0.0~1.0 (0.6 ì´ìƒ = ë‚´ìš© ìœ ì‚¬, 0.3 ~ 0.6 = ë¶€ë¶„ ìœ ì‚¬ / í‘œí˜„ ì°¨ì´, 0.3 ë¯¸ë§Œ = ë¬´ê´€)

---

### **ë¶„ì„ ìš”ì²­**
ê° í›„ë³´ ì¡°í•­ì„ í‘œì¤€ ì¡°í•­ê³¼ ë¹„êµí•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.

**ì‘ì„± ê°€ì´ë“œ:**
1. **ê·¼ê±°(reasoning)**: 
   - **ë°˜ë“œì‹œ í›„ë³´ ì¡°í•­ì˜ ì‹¤ì œ ë‚´ìš©ì„ ì§ì ‘ ì¸ìš©**í•˜ì—¬ ë¹„êµí•˜ì„¸ìš”
   - í‘œì¤€ ì¡°í•­ì˜ í•µì‹¬ ìš”ì†Œ(ë‚´ìš©, ì¡°ê±´, ì ˆì°¨ ë“±)ë¥¼ íŒŒì•…í•˜ê³ , ê° í›„ë³´ê°€ ì´ë¥¼ ì–¼ë§ˆë‚˜ í¬í•¨í•˜ëŠ”ì§€ ì„œìˆ 
   - ì˜ˆì‹œ: "í›„ë³´ ì¡°í•­ì€ 'ë°ì´í„° ì œê³µ ë²”ìœ„ëŠ” ë³„ë„ í•©ì˜'ë¼ê³  ëª…ì‹œí•˜ê³  ìˆì–´, í‘œì¤€ì˜ 'ë³„ì§€1ì— ê¸°ì¬' ë°©ì‹ê³¼ ìœ ì‚¬í•˜ë‚˜..."
   - ëˆ„ë½ëœ ë¶€ë¶„ì´ ìˆë‹¤ë©´ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œ (2-3ë¬¸ì¥ì˜ ê°„ê²°í•œ ë¬¸ë‹¨)

2. **ìœ„í—˜(risk)**: "í•´ë‹¹ ì¡°í•­ì´ ì—†ìœ¼ë©´.." í˜•ì‹ì˜ ì‹œë‚˜ë¦¬ì˜¤ë¡œ ì‘ì„±í•˜ì„¸ìš”. ê³„ì•½ ì²´ê²°Â·ì´í–‰Â·ë¶„ìŸ ì‹œ ë°œìƒí•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ë¬¸ì œ ìƒí™©ì„ ì„œìˆ í•˜ì„¸ìš” (1-2ë¬¸ì¥ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì„œìˆ )

3. **ì¢…í•© ë¶„ì„(summary)**: 
   - Top-3 í›„ë³´ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê²€í† í•œ ê²°ê³¼ë¥¼ ë¬¸ë‹¨ìœ¼ë¡œ ì‘ì„±
   - **ê° í›„ë³´ì˜ í•µì‹¬ ë‚´ìš©ì„ ê°„ëµíˆ ì¸ìš©**í•˜ë©´ì„œ ë¹„êµ
   - ìµœì¢…ì ìœ¼ë¡œ í‘œì¤€ ì¡°í•­ì´ ëˆ„ë½ìœ¼ë¡œ íŒë‹¨ë˜ì—ˆëŠ”ì§€ ì„œìˆ  (3-5ë¬¸ì¥ì˜ ê°„ê²°í•œ ë¬¸ë‹¨)

4. **ì „ì²´ ìœ„í—˜(overall_risk)**: "í•´ë‹¹ ì¡°í•­ì´ ì—†ìœ¼ë©´.." í˜•ì‹ìœ¼ë¡œ ì‹œì‘í•˜ì—¬, ê³„ì•½ì˜ ì „ì²´ ê´€ì ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë²•ì Â·ìš´ì˜ìƒ ìœ„í—˜ì„ ì‹œë‚˜ë¦¬ì˜¤ í˜•ì‹ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš” (2-3ë¬¸ì¥ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨)

5. **ê¶Œê³ (recommendation)**: ê° í›„ë³´ë³„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ "~ë¥¼ ì¶”ê°€í•  ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤" í˜•ì‹ìœ¼ë¡œ í•˜ë‚˜ì˜ ê¶Œê³ ì‚¬í•­ì„ ì‘ì„±í•˜ì„¸ìš” (1-2ë¬¸ì¥)

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”(Top-3 í›„ë³´ ëª¨ë‘ í¬í•¨):
{{
    "candidates": [
        {{
            "candidate_id": "í›„ë³´ ì¡°í•­ ID",
            "is_match": true/false,
            "confidence": 0.0~1.0,
            "match_type": "ë¶€ë¶„ ì¼ì¹˜(í‘œí˜„ ì°¨ì´)" | "ë¬´ê´€",
            "reasoning": "í›„ë³´ ì¡°í•­ì˜ ì‹¤ì œ ë‚´ìš©ì„ ì§ì ‘ ì¸ìš©í•˜ë©° í‘œì¤€ ì¡°í•­ê³¼ ë¹„êµ. ì˜ˆ: 'í›„ë³´ëŠ” \\"[ì‹¤ì œ ë¬¸êµ¬]\\"ë¼ê³  ëª…ì‹œí•˜ì—¬...' í˜•ì‹ìœ¼ë¡œ ì‘ì„± (2-3ë¬¸ì¥)",
            "risk": "í•´ë‹¹ ì¡°í•­ì´ ì—†ìœ¼ë©´ [êµ¬ì²´ì  ë¬¸ì œ ìƒí™©]ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [ì¶”ê°€ ìœ„í—˜ ì„œìˆ ] (1-2ë¬¸ì¥)",
            "recommendation": "êµ¬ì²´ì  ê¶Œê³ ì‚¬í•­ì„ ì„œìˆ í•˜ê³  '~ë¥¼ ì¶”ê°€í•  ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤'ë¡œ ë§ˆë¬´ë¦¬(1-2ë¬¸ì¥)"
        }},
        {{
            "candidate_id": "í›„ë³´ 2 ì¡°í•­ ID",
            "is_match": true/false,
            "confidence": 0.0~1.0,
            "match_type": "ë¶€ë¶„ ì¼ì¹˜(í‘œí˜„ ì°¨ì´)" | "ë¬´ê´€",
            "reasoning": "í›„ë³´ ì¡°í•­ ë‚´ìš©ì„ ì§ì ‘ ì¸ìš©í•˜ë©° ë¹„êµ (2-3ë¬¸ì¥)",
            "risk": "í•´ë‹¹ ì¡°í•­ì´ ì—†ìœ¼ë©´.. ì‹œë‚˜ë¦¬ì˜¤ (1-2ë¬¸ì¥)",
            "recommendation": "~ë¥¼ ì¶”ê°€í•  ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤(1-2ë¬¸ì¥)"
        }},
        {{
            "candidate_id": "í›„ë³´ 3 ì¡°í•­ ID",
            "is_match": true/false,
            "confidence": 0.0~1.0,
            "match_type": "ë¶€ë¶„ ì¼ì¹˜(í‘œí˜„ ì°¨ì´)" | "ë¬´ê´€",
            "reasoning": "í›„ë³´ ì¡°í•­ ë‚´ìš©ì„ ì§ì ‘ ì¸ìš©í•˜ë©° ë¹„êµ (2-3ë¬¸ì¥)",
            "risk": "í•´ë‹¹ ì¡°í•­ì´ ì—†ìœ¼ë©´.. ì‹œë‚˜ë¦¬ì˜¤ (1-2ë¬¸ì¥)",
            "recommendation": "~ë¥¼ ì¶”ê°€í•  ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤(1-2ë¬¸ì¥)"
        }}
    ],
    "summary": "Top-3 í›„ë³´ì˜ í•µì‹¬ ë‚´ìš©ì„ ê°„ëµíˆ ì¸ìš©í•˜ë©° ì¢…í•© ë¹„êµ. ìµœì¢…ì ìœ¼ë¡œ í‘œì¤€ ì¡°í•­ì´ ëˆ„ë½ìœ¼ë¡œ íŒë‹¨ë˜ì—ˆëŠ”ì§€ ì„œìˆ  (3-5ë¬¸ì¥ì˜ ê°„ê²°í•œ ë¬¸ë‹¨)",
    "overall_risk": "í•´ë‹¹ ì¡°í•­ì´ ì—†ìœ¼ë©´ [êµ¬ì²´ì  ì‹œë‚˜ë¦¬ì˜¤]ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê³„ì•½ ì²´ê²°Â·ì´í–‰Â·ë¶„ìŸ ì‹œ ì–´ë–¤ ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆëŠ”ì§€ ìì—°ìŠ¤ëŸ½ê²Œ ì„œìˆ  (2-3ë¬¸ì¥ì˜ ê°„ê²°í•œ ë¬¸ë‹¨)"
}}

JSONë§Œ ì‘ë‹µí•˜ì„¸ìš”."""
        
        return prompt
    
    def _parse_forward_verification_response(
        self,
        response_text: str,
        user_candidates: List[Dict[str, Any]],
        standard_article: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ì—­ë°©í–¥ ê²€ì¦ ì‘ë‹µ íŒŒì‹±
        
        Args:
            response_text: LLM ì‘ë‹µ
            user_candidates: ì‚¬ìš©ì ì¡°ë¬¸ í›„ë³´ë“¤
            standard_article: í‘œì¤€ ì¡°ë¬¸
        
        Returns:
            íŒŒì‹±ëœ ê²€ì¦ ê²°ê³¼
        """
        import json
        
        try:
            data = json.loads(response_text)
            
            # í›„ë³´ë³„ ë¶„ì„ ê²°ê³¼
            candidates_analysis = data.get('candidates', [])
            
            # ë§¤ì¹­ëœ í›„ë³´ ì°¾ê¸° (is_match=Trueì´ê³  confidenceê°€ ê°€ì¥ ë†’ì€ ê²ƒ)
            matched_candidate = None
            matched_user_article = None
            max_confidence = 0.0
            
            for i, candidate_data in enumerate(candidates_analysis):
                if candidate_data.get('is_match', False):
                    confidence = float(candidate_data.get('confidence', 0.0))
                    if confidence > max_confidence:
                        max_confidence = confidence
                        matched_candidate = candidate_data
                        if i < len(user_candidates):
                            matched_user_article = user_candidates[i]['user_article']
            
            # ì‹¤ì œ ëˆ„ë½ ì—¬ë¶€ íŒë‹¨
            is_truly_missing = matched_candidate is None
            
            # ì¢…í•© ë¶„ì„ì—ì„œ ì •ë³´ ì¶”ì¶œ
            summary = data.get('summary', '')
            overall_risk = data.get('overall_risk', '')
            
            # ê¶Œê³ ì‚¬í•­ (ë§¤ì¹­ëœ í›„ë³´ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì˜ ê¶Œê³ , ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ í›„ë³´ì˜ ê¶Œê³ )
            if matched_candidate:
                recommendation = matched_candidate.get('recommendation', f"'{standard_article.get('title')}' ì¡°í•­ í™•ì¸ í•„ìš”")
                reasoning = matched_candidate.get('reasoning', '')
            elif candidates_analysis:
                recommendation = candidates_analysis[0].get('recommendation', f"'{standard_article.get('title')}' ì¡°í•­ì„ ì¶”ê°€í•  ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
                reasoning = summary
            else:
                recommendation = f"'{standard_article.get('title')}' ì¡°í•­ì„ ì¶”ê°€í•  ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤."
                reasoning = "í›„ë³´ ì¡°ë¬¸ ë¶„ì„ ê²°ê³¼ ì—†ìŒ"
            
            return {
                "is_truly_missing": is_truly_missing,
                "confidence": max_confidence if matched_candidate else 1.0,
                "matched_user_article": matched_user_article,
                "reasoning": reasoning,
                "recommendation": recommendation,
                "evidence": summary,
                "risk_assessment": overall_risk,
                "candidates_analysis": candidates_analysis
            }
            
        except json.JSONDecodeError as e:
            logger.warning(f"    JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            return {
                "is_truly_missing": True,
                "confidence": 0.5,
                "matched_user_article": None,
                "reasoning": "JSON íŒŒì‹± ì‹¤íŒ¨",
                "recommendation": f"'{standard_article.get('title')}' ì¡°í•­ í™•ì¸ í•„ìš”",
                "evidence": response_text,
                "risk_assessment": "íŒŒì‹± ì‹¤íŒ¨ë¡œ ì •í™•í•œ í‰ê°€ ë¶ˆê°€",
                "candidates_analysis": []
            }

    def _generate_missing_analysis_without_candidates(
        self,
        standard_article: Dict[str, Any],
        contract_type: str
    ) -> Dict[str, Any]:
        """
        í›„ë³´ ì¡°ë¬¸ì´ ì—†ì„ ë•Œ LLMìœ¼ë¡œ ìƒì„¸í•œ ëˆ„ë½ ë¶„ì„ ìƒì„±
        
        Args:
            standard_article: ëˆ„ë½ëœ í‘œì¤€ ì¡°ë¬¸
            contract_type: ê³„ì•½ ìœ í˜•
        
        Returns:
            ìƒì„¸ ë¶„ì„ ê²°ê³¼
        """
        parent_id = standard_article.get('parent_id')
        title = standard_article.get('title', '')
        standard_text = self._format_standard_article(standard_article)
        
        contract_type_names = {
            "provide": "ë°ì´í„° ì œê³µ ê³„ì•½",
            "create": "ë°ì´í„° ìƒì„± ê³„ì•½",
            "process": "ë°ì´í„° ê°€ê³µ ê³„ì•½",
            "brokerage_provider": "ë°ì´í„° ì¤‘ê°œ ê³„ì•½ (ì œê³µììš©)",
            "brokerage_user": "ë°ì´í„° ì¤‘ê°œ ê³„ì•½ (ì´ìš©ììš©)"
        }
        
        contract_name = contract_type_names.get(contract_type, contract_type)
        
        prompt = f"""ë‹¹ì‹ ì€ ê³„ì•½ì„œ ì¡°í•­ ë¶„ì„ ë° ë¦¬ìŠ¤í¬ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ë¶„ì„ ëŒ€ìƒ**
{contract_name}ì˜ í‘œì¤€ê³„ì•½ì„œ ì¡°í•­ "{parent_id} ({title})"ì´ ì‚¬ìš©ì ê³„ì•½ì„œì—ì„œ ì™„ì „íˆ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.
ì—­ë°©í–¥ ê²€ìƒ‰ì—ì„œë„ ìœ ì‚¬í•œ ì¡°ë¬¸ì„ ì „í˜€ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.

**í‘œì¤€ ì¡°í•­ ({parent_id}):**
{standard_text}

---

**ê³¼ì œ**: ì´ ì¡°í•­ì´ ëˆ„ë½ë˜ì—ˆì„ ë•Œì˜ ì˜í–¥ì„ ìƒì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”.

**ì‘ì„± ê°€ì´ë“œ:**
1. **ì¡°í•­ì˜ í•µì‹¬ ëª©ì **: ì´ ì¡°í•­ì´ ê³„ì•½ì„œì—ì„œ ìˆ˜í–‰í•˜ëŠ” í•µì‹¬ ì—­í• ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…

2. **ëˆ„ë½ìœ¼ë¡œ ì¸í•œ ìœ„í—˜**: "í•´ë‹¹ ì¡°í•­ì´ ì—†ìœ¼ë©´..." í˜•ì‹ìœ¼ë¡œ ì‹œì‘í•˜ì—¬, ê³„ì•½ ì²´ê²°Â·ì´í–‰Â·ë¶„ìŸ ì‹œ ë°œìƒí•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ë¬¸ì œ ìƒí™©ì„ ì‹œë‚˜ë¦¬ì˜¤ í˜•ì‹ìœ¼ë¡œ ì„œìˆ  (3-4ë¬¸ì¥)

3. **ë²•ì Â·ìš´ì˜ìƒ ì˜í–¥**: ì´ ì¡°í•­ì˜ ë¶€ì¬ê°€ ê³„ì•½ ì „ì²´ì— ë¯¸ì¹˜ëŠ” ë²•ì Â·ìš´ì˜ìƒ ì˜í–¥ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„œìˆ  (2-3ë¬¸ì¥)

4. **ê¶Œê³ ì‚¬í•­**: ì´ ì¡°í•­ì„ ì–´ë–»ê²Œ ì¶”ê°€í•´ì•¼ í•˜ëŠ”ì§€ êµ¬ì²´ì ì¸ ê¶Œê³  (2-3ë¬¸ì¥)

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "purpose": "ì¡°í•­ì˜ í•µì‹¬ ëª©ì  (2-3ë¬¸ì¥)",
    "risk_scenario": "í•´ë‹¹ ì¡°í•­ì´ ì—†ìœ¼ë©´ [êµ¬ì²´ì  ì‹œë‚˜ë¦¬ì˜¤]ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [ì¶”ê°€ ìœ„í—˜ ì„œìˆ ] (3-4ë¬¸ì¥)",
    "legal_impact": "ë²•ì Â·ìš´ì˜ìƒ ì˜í–¥ ë¶„ì„ (2-3ë¬¸ì¥)",
    "recommendation": "êµ¬ì²´ì  ê¶Œê³ ì‚¬í•­. '~ë¥¼ ì¶”ê°€í•  ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤'ë¡œ ë§ˆë¬´ë¦¬ (2-3ë¬¸ì¥)"
}}

JSONë§Œ ì‘ë‹µí•˜ì„¸ìš”."""
        
        try:
            response = self.azure_client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ ê³„ì•½ì„œ ì¡°í•­ì„ ì •í™•í•˜ê²Œ ë¶„ì„í•˜ëŠ” ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.1,
                max_tokens=1000,
                response_format={"type": "json_object"}
            )
            
            response_text = response.choices[0].message.content.strip()
            usage = response.usage
            
            # JSON íŒŒì‹±
            import json
            data = json.loads(response_text)
            
            # ê²°ê³¼ êµ¬ì„±
            purpose = data.get('purpose', '')
            risk_scenario = data.get('risk_scenario', '')
            legal_impact = data.get('legal_impact', '')
            recommendation = data.get('recommendation', f"'{title}' ì¡°í•­ì„ ì¶”ê°€í•  ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            
            # ì¦ê±° í…ìŠ¤íŠ¸ êµ¬ì„±
            evidence = f"""**ì¡°í•­ì˜ í•µì‹¬ ëª©ì :**
{purpose}

**ì—­ë°©í–¥ ê²€ìƒ‰ ê²°ê³¼:**
ì‚¬ìš©ì ê³„ì•½ì„œ ì „ì²´ë¥¼ ê²€ìƒ‰í–ˆìœ¼ë‚˜ ì´ ì¡°í•­ê³¼ ìœ ì‚¬í•œ ë‚´ìš©ì„ ì „í˜€ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.

**ë²•ì Â·ìš´ì˜ìƒ ì˜í–¥:**
{legal_impact}"""
            
            logger.info(f"    LLM ëˆ„ë½ ë¶„ì„ ì™„ë£Œ (í† í°: {usage.total_tokens})")
            
            return {
                "is_truly_missing": True,
                "confidence": 1.0,
                "matched_user_article": None,
                "reasoning": f"ì—­ë°©í–¥ ê²€ìƒ‰ì—ì„œ ìœ ì‚¬í•œ ì¡°ë¬¸ì„ ì „í˜€ ì°¾ì„ ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. {purpose}",
                "recommendation": recommendation,
                "evidence": evidence,
                "risk_assessment": risk_scenario,
                "candidates_analysis": [],
                "prompt_tokens": usage.prompt_tokens,
                "completion_tokens": usage.completion_tokens
            }
            
        except Exception as e:
            logger.error(f"    LLM ëˆ„ë½ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "is_truly_missing": True,
                "confidence": 1.0,
                "matched_user_article": None,
                "reasoning": "ì‚¬ìš©ì ê³„ì•½ì„œì—ì„œ ìœ ì‚¬í•œ ì¡°ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "recommendation": f"'{title}' ì¡°í•­ì„ ì¶”ê°€í•  ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.",
                "evidence": "ì—­ë°©í–¥ ê²€ìƒ‰ì—ì„œ ìœ ì‚¬í•œ ì‚¬ìš©ì ì¡°ë¬¸ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "risk_assessment": "í•´ë‹¹ ì¡°í•­ì´ ì—†ìœ¼ë©´ ê³„ì•½ ì´í–‰ ê³¼ì •ì—ì„œ ë¶ˆëª…í™•ì„±ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "candidates_analysis": [],
                "prompt_tokens": 0,
                "completion_tokens": 0
            }

    def set_all_chunks(self, contract_type: str):
        """
        ì „ì²´ ì²­í¬ ë¡œë“œ (ì°¸ì¡° í•´ê²°ìš©)
        
        Args:
            contract_type: ê³„ì•½ ìœ í˜•
        """
        if self.kb_loader:
            self.all_chunks = self.kb_loader.load_chunks(contract_type) or []
            logger.debug(f"    ì „ì²´ ì²­í¬ ë¡œë“œ ì™„ë£Œ: {len(self.all_chunks)}ê°œ")
        else:
            logger.warning("    KnowledgeBaseLoaderê°€ ì—†ì–´ ì²­í¬ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            self.all_chunks = []

    def _load_referenced_exhibits(self, references: List[str]) -> Dict[str, str]:
        """
        ë³„ì§€ ì°¸ì¡° ë¡œë“œ (text_llmë§Œ ì‚¬ìš©)

        Args:
            references: ì°¸ì¡° ID ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["urn:std:process:ex:001:idx:001"])

        Returns:
            {chunk_id: text_llm} ë”•ì…”ë„ˆë¦¬
        """
        exhibit_contents = {}

        if not self.all_chunks:
            logger.warning("      ì „ì²´ ì²­í¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•„ ë³„ì§€ ì°¸ì¡°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return exhibit_contents

        for ref_id in references:
            if ':ex:' not in ref_id:
                continue

            # global_idë¡œ ì²­í¬ ì°¾ê¸°
            for chunk in self.all_chunks:
                if chunk.get('global_id') == ref_id:
                    chunk_id = chunk.get('id', ref_id)
                    text_llm = chunk.get('text_llm', '').strip()
                    
                    if text_llm:
                        exhibit_contents[chunk_id] = text_llm
                        logger.debug(f"        ë³„ì§€ ì°¸ì¡° ë¡œë“œ: {chunk_id}")
                    else:
                        logger.warning(f"        ë³„ì§€ {chunk_id}ì— text_llmì´ ì—†ìŠµë‹ˆë‹¤")
                    break

        return exhibit_contents

    def _load_referenced_articles(self, references: List[str]) -> Dict[str, str]:
        """
        ì¡°í•­ ì°¸ì¡° ë¡œë“œ (text_norm + commentary_summary)

        Args:
            references: ì°¸ì¡° ID ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["urn:std:process:art:023:cla:002"])

        Returns:
            {chunk_id: formatted_content} ë”•ì…”ë„ˆë¦¬
        """
        article_contents = {}

        if not self.all_chunks:
            logger.warning("      ì „ì²´ ì²­í¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•„ ì¡°í•­ ì°¸ì¡°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return article_contents

        for ref_id in references:
            if ':art:' not in ref_id:
                continue

            # global_idë¡œ ì²­í¬ ì°¾ê¸°
            for chunk in self.all_chunks:
                if chunk.get('global_id') == ref_id:
                    chunk_id = chunk.get('id', ref_id)
                    text_norm = chunk.get('text_norm', '').strip()
                    commentary_summary = chunk.get('commentary_summary', '').strip()
                    
                    if text_norm:
                        # text_norm + commentary_summary ê²°í•©
                        content_parts = [text_norm]
                        if commentary_summary:
                            content_parts.append(f"[í•´ì„¤] {commentary_summary}")
                        
                        article_contents[chunk_id] = "\n        ".join(content_parts)
                        logger.debug(f"        ì¡°í•­ ì°¸ì¡° ë¡œë“œ: {chunk_id}")
                    else:
                        logger.warning(f"        ì¡°í•­ {chunk_id}ì— text_normì´ ì—†ìŠµë‹ˆë‹¤")
                    break

        return article_contents
