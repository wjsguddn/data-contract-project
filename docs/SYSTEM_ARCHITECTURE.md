<<<<<<< HEAD
# ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨

## ì „ì²´ ì‹œìŠ¤í…œ êµ¬ì¡°

```mermaid
graph TB
    subgraph "Frontend Layer"
        UI[Streamlit UI<br/>í¬íŠ¸ 8501]
    end

    subgraph "API Layer"
        API[FastAPI Server<br/>í¬íŠ¸ 8000]
    end

    subgraph "Message Queue"
        REDIS[Redis<br/>í¬íŠ¸ 6379]
    end

    subgraph "Agent Workers"
        CLS[Classification Agent<br/>Celery Worker]
        CONS[Consistency Agent<br/>Celery Worker]
        RPT[Report Agent<br/>Celery Worker]
    end

    subgraph "Data Storage"
        DB[(SQLite Database)]
        FILES[File Storage<br/>data/]
        INDEXES[Search Indexes<br/>FAISS/Whoosh]
    end

    subgraph "Knowledge Base"
        STD[í‘œì¤€ê³„ì•½ì„œ 5ì¢…]
        GUIDE[í™œìš©ì•ˆë‚´ì„œ]
    end

    UI -->|HTTP Request| API
    API -->|Enqueue Task| REDIS
    REDIS -->|Consume Task| CLS
    REDIS -->|Consume Task| CONS
    REDIS -->|Consume Task| RPT
    
    CLS -->|Query| INDEXES
    CONS -->|Query| INDEXES
    RPT -->|Query| INDEXES
    
    CLS -->|Update Status| DB
    CONS -->|Update Status| DB
    RPT -->|Update Status| DB
    
    API -->|Read/Write| DB
    API -->|Store Files| FILES
    
    STD -->|Indexed| INDEXES
    GUIDE -->|Indexed| INDEXES
```

## ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

```mermaid
flowchart LR
    subgraph "Ingestion Pipeline"
        SRC[ì›ë³¸ ë¬¸ì„œ<br/>DOCX/PDF]
        PARSE[Parser<br/>êµ¬ì¡° ì¶”ì¶œ]
        CHUNK[Chunker<br/>ì¡° ë‹¨ìœ„ ë¶„í• ]
        EMBED[Embedder<br/>ë²¡í„° ìƒì„±]
        INDEX[Indexer<br/>ê²€ìƒ‰ ì¸ë±ìŠ¤]
    end

    SRC --> PARSE
    PARSE --> CHUNK
    CHUNK --> EMBED
    EMBED --> INDEX

    PARSE -.->|JSON| EXT[extracted_documents/]
    CHUNK -.->|JSONL| CHK[chunked_documents/]
    INDEX -.->|FAISS/Whoosh| IDX[search_indexes/]
```

## ì‚¬ìš©ì ê³„ì•½ì„œ ë¶„ì„ í”Œë¡œìš°
=======
# ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

> **ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-11-03
> **ë¶„ì„ ê¸°ì¤€**: ì‹¤ì œ ì½”ë“œë² ì´ìŠ¤ ê²€ì¦ (ë¬¸ì„œ ì•„ë‹˜)

ë³¸ ë¬¸ì„œëŠ” ë°ì´í„° ê³„ì•½ì„œ ê²€ì¦ ì‹œìŠ¤í…œì˜ ì „ì²´ ì•„í‚¤í…ì²˜ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.

## ëª©ì°¨
1. [ì‹œìŠ¤í…œ ê°œìš”](#ì‹œìŠ¤í…œ-ê°œìš”)
2. [ì „ì²´ ì•„í‚¤í…ì²˜](#ì „ì²´-ì•„í‚¤í…ì²˜)
3. [ì»¨í…Œì´ë„ˆ êµ¬ì„±](#ì»¨í…Œì´ë„ˆ-êµ¬ì„±)
4. [ë°ì´í„° íë¦„](#ë°ì´í„°-íë¦„)
5. [ê²€ìƒ‰ ì•„í‚¤í…ì²˜](#ê²€ìƒ‰-ì•„í‚¤í…ì²˜)
6. [ë°ì´í„° ì €ì¥ì†Œ](#ë°ì´í„°-ì €ì¥ì†Œ)
7. [AI/ML í†µí•©](#aiml-í†µí•©)

---

## ì‹œìŠ¤í…œ ê°œìš”

**í”„ë¡œì íŠ¸ëª…**: ë°ì´í„° ê³„ì•½ì„œ ê²€ì¦ í”Œë«í¼
**ëª©ì **: ì‚¬ìš©ì ê³„ì•½ì„œë¥¼ 5ê°œ í‘œì¤€ ê³„ì•½ì„œ ìœ í˜•ìœ¼ë¡œ ìë™ ë¶„ë¥˜í•˜ê³ , RAG ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ì¼ê´€ì„±ì„ ê²€ì¦
**ê¸°ìˆ  ìŠ¤íƒ**: FastAPI, Streamlit, Celery, Redis, SQLite, FAISS, Whoosh, Azure OpenAI
**ì•„í‚¤í…ì²˜ íŒ¨í„´**: ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ + ë¹„ë™ê¸° íƒœìŠ¤í¬ ì²˜ë¦¬ + RAG (Retrieval-Augmented Generation)

---

## ì „ì²´ ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph "í´ë¼ì´ì–¸íŠ¸"
        UI[Streamlit Frontend<br/>íŒŒì¼ ì—…ë¡œë“œ, ê²°ê³¼ í‘œì‹œ]
    end

    subgraph "Docker Compose í™˜ê²½"
        subgraph "API Layer"
            API[FastAPI Backend<br/>:8000<br/>REST API ì„œë²„]
        end

        subgraph "Task Queue"
            Redis[Redis<br/>:6379<br/>Broker & Result Backend]
        end

        subgraph "Celery Workers"
            CW1[Classification Worker<br/>Queue: classification<br/>ê³„ì•½ì„œ ë¶„ë¥˜]
            CW2[Consistency Worker<br/>Queue: consistency_validation<br/>ì¼ê´€ì„± ê²€ì¦ A1, A2, A3]
            CW3[Report Worker<br/>Queue: report<br/>ë³´ê³ ì„œ ìƒì„± stub]
        end

        subgraph "ë°ì´í„° ì €ì¥ì†Œ"
            DB[(SQLite Database<br/>contracts.db<br/>5ê°œ í…Œì´ë¸”)]
            FS[File Storage<br/>data/<br/>íŒŒì‹± ê²°ê³¼, ì‚¬ìš©ì ê³„ì•½ì„œ]
        end

        subgraph "ê²€ìƒ‰ ì—”ì§„"
            FAISS[FAISS Vector Indexes<br/>search_indexes/faiss/<br/>10ê°œ ì¸ë±ìŠ¤<br/>text + title ì´ì¤‘í™”]
            Whoosh[Whoosh Keyword Indexes<br/>search_indexes/whoosh/<br/>5ê°œ ì¸ë±ìŠ¤<br/>í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„]
        end
    end

    subgraph "ì™¸ë¶€ ì„œë¹„ìŠ¤"
        Azure[Azure OpenAI<br/>GPT-4o<br/>text-embedding-3-large]
    end

    subgraph "ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶• CLI"
        Ingestion[Ingestion CLI<br/>docker-compose --profile ingestion<br/>íŒŒì‹±, ì²­í‚¹, ì„ë² ë”©, ì¸ë±ì‹±]
    end

    %% ì‚¬ìš©ì í”Œë¡œìš°
    UI -->|HTTP POST /upload| API
    UI -->|HTTP GET /api/classification/:id| API
    UI -->|HTTP POST /api/validation/:id/start| API
    API -->|ì‘ë‹µ| UI

    %% ë°±ì—”ë“œ â†’ í
    API -->|Celery Task ë°œí–‰| Redis
    Redis -->|Task ë°°í¬| CW1
    Redis -->|Task ë°°í¬| CW2
    Redis -->|Task ë°°í¬| CW3

    %% ì›Œì»¤ â†’ DB/íŒŒì¼
    CW1 <-->|Read/Write| DB
    CW2 <-->|Read/Write| DB
    CW3 <-->|Read/Write| DB
    API <-->|SQLAlchemy ORM| DB
    API -->|íŒŒì‹± ê²°ê³¼ ì €ì¥| FS

    %% ì›Œì»¤ â†’ ê²€ìƒ‰
    CW1 -->|ì„ë² ë”© ìœ ì‚¬ë„ ê²€ìƒ‰| FAISS
    CW2 -->|í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰| FAISS
    CW2 -->|í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰| Whoosh

    %% AI ì„œë¹„ìŠ¤ í˜¸ì¶œ
    API -->|ì„ë² ë”© ìƒì„±| Azure
    CW1 -->|LLM ë¶„ë¥˜ Few-shot| Azure
    CW2 -->|LLM ê²€ì¦/ë¶„ì„| Azure

    %% ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•
    Ingestion -->|í‘œì¤€ ê³„ì•½ì„œ íŒŒì‹±| FS
    Ingestion -->|ì„ë² ë”© ìƒì„±| Azure
    Ingestion -->|ì¸ë±ìŠ¤ ìƒì„±| FAISS
    Ingestion -->|ì¸ë±ìŠ¤ ìƒì„±| Whoosh

    %% ìŠ¤íƒ€ì¼ë§
    classDef frontend fill:#e1f5ff,stroke:#01579b,stroke-width:2px
    classDef backend fill:#fff9c4,stroke:#f57f17,stroke-width:2px
    classDef worker fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef storage fill:#e8f5e9,stroke:#1b5e20,stroke-width:2px
    classDef external fill:#ffebee,stroke:#b71c1c,stroke-width:2px
    classDef queue fill:#fff3e0,stroke:#e65100,stroke-width:2px

    class UI frontend
    class API backend
    class CW1,CW2,CW3 worker
    class DB,FS,FAISS,Whoosh storage
    class Azure external
    class Redis queue
    class Ingestion backend
```

---

## ì»¨í…Œì´ë„ˆ êµ¬ì„±

### Docker Compose Services

| ì„œë¹„ìŠ¤ëª… | ì´ë¯¸ì§€ | í¬íŠ¸ | ì—­í•  | ì˜ì¡´ì„± | ìƒíƒœ |
|---------|-------|-----|-----|--------|------|
| **fast-api** | Dockerfile.backend | 8000 | FastAPI REST API ì„œë²„ | redis | âœ… ì‹¤í–‰ ì¤‘ |
| **classification-worker** | Dockerfile.classification | - | ê³„ì•½ì„œ ë¶„ë¥˜ Celery Worker | redis | âœ… ì‹¤í–‰ ì¤‘ |
| **consistency-validation-worker** | Dockerfile.consistency | - | ì¼ê´€ì„± ê²€ì¦ Celery Worker | redis | âœ… ì‹¤í–‰ ì¤‘ |
| **report-worker** | Dockerfile.report | - | ë³´ê³ ì„œ ìƒì„± Celery Worker | redis | âš ï¸ Stub |
| **redis** | redis:7-alpine | 6379 | Celery Broker/Backend | - | âœ… ì‹¤í–‰ ì¤‘ |
| **ingestion** | Dockerfile.ingestion | - | ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶• CLI | - | ğŸ”§ Profile ì„œë¹„ìŠ¤ |

**ì°¸ê³ **:
- Streamlit FrontendëŠ” Docker Composeì— í¬í•¨ë˜ì§€ ì•ŠìŒ (ë³„ë„ ì‹¤í–‰)
- Ingestionì€ `--profile ingestion` í”Œë˜ê·¸ë¡œ ìˆ˜ë™ ì‹¤í–‰

### ë³¼ë¥¨ ê³µìœ 

```yaml
volumes:
  - ./data:/app/data                          # íŒŒì‹± ê²°ê³¼, DB, ì‚¬ìš©ì ê³„ì•½ì„œ
  - ./search_indexes:/app/search_indexes      # FAISS, Whoosh ì¸ë±ìŠ¤
  - ./backend:/app/backend                    # ì½”ë“œ í•« ë¦¬ë¡œë“œ (ê°œë°œ)
  - ./ingestion:/app/ingestion                # ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶• ì½”ë“œ
  - redis_data:/data                          # Redis ì˜ì†í™”
```

---

## ë°ì´í„° íë¦„

### 1. ì‚¬ìš©ì ê³„ì•½ì„œ ì²˜ë¦¬ í”Œë¡œìš°
>>>>>>> c7c7d8f082d58f15a66cf160f5d601f1ab908b93

```mermaid
sequenceDiagram
    actor User
<<<<<<< HEAD
    participant UI as Streamlit UI
    participant API as FastAPI
    participant Redis
    participant CLS as Classification<br/>Agent
    participant CONS as Consistency<br/>Agent
    participant RPT as Report<br/>Agent
    participant DB as Database
    participant KB as Knowledge Base

    User->>UI: ê³„ì•½ì„œ ì—…ë¡œë“œ
    UI->>API: POST /upload
    API->>DB: ê³„ì•½ì„œ ì €ì¥
    API->>Redis: Enqueue ë¶„ë¥˜ ì‘ì—…
    API-->>UI: task_id ë°˜í™˜
    
    Redis->>CLS: ì‘ì—… í• ë‹¹
    CLS->>KB: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    KB-->>CLS: ìœ ì‚¬ ì¡°í•­ ë°˜í™˜
    CLS->>CLS: LLM ë¶„ë¥˜ íŒë‹¨
    CLS->>DB: ë¶„ë¥˜ ê²°ê³¼ ì €ì¥
    CLS->>Redis: Enqueue ê²€ì¦ ì‘ì—…
    
    Redis->>CONS: ì‘ì—… í• ë‹¹
    CONS->>CONS: A1: ì¡°í•­ ë§¤ì¹­
    CONS->>KB: í‘œì¤€ê³„ì•½ì„œ ì¡°íšŒ
    CONS->>CONS: A2: ì²´í¬ë¦¬ìŠ¤íŠ¸ ê²€ì¦
    CONS->>KB: ì²´í¬ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ
    CONS->>CONS: A3: ë‚´ìš© ë¹„êµ
    CONS->>KB: ì˜ë¯¸ ìœ ì‚¬ë„ ê²€ìƒ‰
    CONS->>DB: ê²€ì¦ ê²°ê³¼ ì €ì¥
    CONS->>Redis: Enqueue ë³´ê³ ì„œ ì‘ì—…
    
    Redis->>RPT: ì‘ì—… í• ë‹¹
    RPT->>DB: ë¶„ì„ ê²°ê³¼ ì¡°íšŒ
    RPT->>RPT: ë³´ê³ ì„œ ìƒì„±
    RPT->>DB: ë³´ê³ ì„œ ì €ì¥
    
    UI->>API: GET /status/{task_id}
    API->>DB: ìƒíƒœ ì¡°íšŒ
    API-->>UI: ê²°ê³¼ ë°˜í™˜
    UI-->>User: ë¶„ì„ ë³´ê³ ì„œ í‘œì‹œ
```

## Classification Agent ë‚´ë¶€ êµ¬ì¡°

```mermaid
flowchart TD
    START([ë¶„ë¥˜ ì‘ì—… ì‹œì‘])
    PARSE[ì‚¬ìš©ì ê³„ì•½ì„œ íŒŒì‹±]
    CHUNK[ì¡° ë‹¨ìœ„ ì²­í‚¹]
    
    subgraph "RAG ê²€ìƒ‰"
        VSEARCH[ë²¡í„° ê²€ìƒ‰<br/>FAISS]
        KSEARCH[í‚¤ì›Œë“œ ê²€ìƒ‰<br/>Whoosh]
        HYBRID[í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°]
    end
    
    LLM[LLM ë¶„ë¥˜ íŒë‹¨<br/>GPT-4]
    SAVE[ë¶„ë¥˜ ê²°ê³¼ ì €ì¥]
    NEXT[ë‹¤ìŒ ë‹¨ê³„ íŠ¸ë¦¬ê±°]
    END([ì™„ë£Œ])

    START --> PARSE
    PARSE --> CHUNK
    CHUNK --> VSEARCH
    CHUNK --> KSEARCH
    VSEARCH --> HYBRID
    KSEARCH --> HYBRID
    HYBRID --> LLM
    LLM --> SAVE
    SAVE --> NEXT
    NEXT --> END
```

## Consistency Agent ë‚´ë¶€ êµ¬ì¡°

```mermaid
flowchart TD
    START([ê²€ì¦ ì‘ì—… ì‹œì‘])
    LOAD[ë¶„ë¥˜ ê²°ê³¼ ë¡œë“œ]
    
    subgraph "A1 Node: ì¡°í•­ ë§¤ì¹­ (ê¸°ì¤€ ìƒì„±)"
        A1_QUERY[Ingestion ì¸ë±ìŠ¤ ê²€ìƒ‰<br/>FAISS + Whoosh]
        A1_MATCH[ì¡°í•­ 1ì°¨ ë§¤ì¹­<br/>í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°]
        A1_SAVE[ë§¤ì¹­ ê²°ê³¼ JSON ì €ì¥<br/>article_mapping.json]
        A1_CHECK{ëˆ„ë½ ì¡°í•­<br/>ì¡´ì¬?}
        A1_REVERIFY[ëˆ„ë½ ì¡°í•­ ì¬ê²€ì¦<br/>LLM íŒë‹¨]
        A1_FINAL[ì¬ê²€ì¦ ê²°ê³¼ ë°˜ì˜<br/>ìµœì¢… ì™„ë£Œ]
    end
    
    subgraph "A2 Node: ì²´í¬ë¦¬ìŠ¤íŠ¸ ê²€ì¦ (ë§¤ì¹­ëœ ì¡°í•­ë§Œ)"
        A2_LOAD_MAP[article_mapping.json ë¡œë“œ]
        A2_FILTER[matched ì¡°í•­ë§Œ í•„í„°ë§]
        A2_LOAD_CHECK[ì²´í¬ë¦¬ìŠ¤íŠ¸ ë¡œë“œ<br/>Ingestion ì¸ë±ìŠ¤]
        A2_VERIFY[ë§¤ì¹­ëœ ì¡°í•­ ê¸°ì¤€<br/>í•­ëª©ë³„ ê²€ì¦]
        A2_RESULT[ê²€ì¦ ê²°ê³¼ ìƒì„±<br/>checklist_result.json]
    end
    
    subgraph "A3 Node: ë‚´ìš© ë¹„êµ (ë§¤ì¹­ëœ ì¡°í•­ë§Œ)"
        A3_LOAD_MAP[article_mapping.json ë¡œë“œ]
        A3_FILTER[matched ì¡°í•­ë§Œ í•„í„°ë§]
        A3_COMPARE[ì¡°í•­ ìŒë³„ ë‚´ìš© ë¹„êµ<br/>ì˜ë¯¸ ìœ ì‚¬ë„ ë¶„ì„]
        A3_ANALYZE[ì°¨ì´ì  ë¶„ì„<br/>LLM íŒë‹¨]
        A3_RESULT[ë¹„êµ ê²°ê³¼ ìƒì„±<br/>content_comparison.json]
    end
    
    MERGE[3ê°œ ê²°ê³¼ í†µí•©]
    SAVE[ìµœì¢… ê²€ì¦ ê²°ê³¼ ì €ì¥<br/>DB]
    NEXT[ë³´ê³ ì„œ ìƒì„± íŠ¸ë¦¬ê±°]
    END([ì™„ë£Œ])

    START --> LOAD
    LOAD --> A1_QUERY
    A1_QUERY --> A1_MATCH
    A1_MATCH --> A1_SAVE
    
    A1_SAVE --> A2_LOAD_MAP
    A1_SAVE --> A3_LOAD_MAP
    A1_SAVE --> A1_CHECK
    
    A1_CHECK -->|Yes| A1_REVERIFY
    A1_CHECK -->|No| A1_FINAL
    A1_REVERIFY --> A1_FINAL
    
    A2_LOAD_MAP --> A2_FILTER
    A2_FILTER --> A2_LOAD_CHECK
    A2_LOAD_CHECK --> A2_VERIFY
    A2_VERIFY --> A2_RESULT
    
    A3_LOAD_MAP --> A3_FILTER
    A3_FILTER --> A3_COMPARE
    A3_COMPARE --> A3_ANALYZE
    A3_ANALYZE --> A3_RESULT
    
    A2_RESULT --> MERGE
    A3_RESULT --> MERGE
    A1_FINAL --> MERGE
    
    MERGE --> SAVE
    SAVE --> NEXT
    NEXT --> END
```

## ë°ì´í„° í”Œë¡œìš°

```mermaid
flowchart LR
    subgraph "ì…ë ¥"
        USER_DOC[ì‚¬ìš©ì ê³„ì•½ì„œ<br/>DOCX]
    end
    
    subgraph "ì§€ì‹ë² ì´ìŠ¤ (ì‚¬ì „ êµ¬ì¶•)"
        STD_DOC[í‘œì¤€ê³„ì•½ì„œ<br/>DOCX]
        GUIDE_DOC[í™œìš©ì•ˆë‚´ì„œ<br/>PDF]
        KB_PROCESS[Ingestion<br/>íŒŒì‹±â†’ì²­í‚¹â†’ì„ë² ë”©]
        KB_INDEX[ê²€ìƒ‰ ì¸ë±ìŠ¤<br/>FAISS + Whoosh]
        
        STD_DOC --> KB_PROCESS
        GUIDE_DOC --> KB_PROCESS
        KB_PROCESS --> KB_INDEX
    end

    subgraph "ì²˜ë¦¬"
        USER_PARSE[íŒŒì‹±]
        USER_JSON[êµ¬ì¡°í™” ë°ì´í„°]
    end

    subgraph "ë¶„ì„"
        CLS_AGENT[Classification<br/>Agent]
        CONS_AGENT[Consistency<br/>Agent]
        RPT_AGENT[Report<br/>Agent]
    end

    subgraph "ì¶œë ¥"
        RESULT[ë¶„ì„ ë³´ê³ ì„œ]
    end

    USER_DOC --> USER_PARSE
    USER_PARSE --> USER_JSON
    
    USER_JSON --> CLS_AGENT
    KB_INDEX --> CLS_AGENT
    
    CLS_AGENT --> CONS_AGENT
    KB_INDEX --> CONS_AGENT
    
    CONS_AGENT --> RPT_AGENT
    RPT_AGENT --> RESULT
```

## Docker ì»¨í…Œì´ë„ˆ êµ¬ì¡°

```mermaid
graph TB
    subgraph "Docker Compose"
        subgraph "Web Services"
            FAST[fast-api<br/>FastAPI Server<br/>8000:8000]
            STREAM[streamlit<br/>Streamlit UI<br/>8501:8501]
        end

        subgraph "Message Queue"
            REDIS_C[redis<br/>Redis Server<br/>6379:6379]
        end

        subgraph "Worker Services"
            CLS_W[classification-worker<br/>Celery Worker<br/>classification queue]
            CONS_W[consistency-worker<br/>Celery Worker<br/>consistency queue]
            RPT_W[report-worker<br/>Celery Worker<br/>report queue]
        end

        subgraph "Utility Services"
            ING[ingestion<br/>Document Processing<br/>profile: ingestion]
        end

        subgraph "Shared Volumes"
            VOL_DATA[./data:/app/data]
            VOL_BACKEND[./backend:/app/backend]
            VOL_INGESTION[./ingestion:/app/ingestion]
        end
    end

    STREAM -.->|HTTP| FAST
    FAST -.->|Redis Protocol| REDIS_C
    CLS_W -.->|Redis Protocol| REDIS_C
    CONS_W -.->|Redis Protocol| REDIS_C
    RPT_W -.->|Redis Protocol| REDIS_C

    FAST -.->|Volume Mount| VOL_DATA
    CLS_W -.->|Volume Mount| VOL_DATA
    CONS_W -.->|Volume Mount| VOL_DATA
    RPT_W -.->|Volume Mount| VOL_DATA
    ING -.->|Volume Mount| VOL_DATA
```

## ê¸°ìˆ  ìŠ¤íƒ ë ˆì´ì–´

```mermaid
graph TB
    subgraph "Presentation Layer"
        ST[Streamlit<br/>Python Web Framework]
    end

    subgraph "API Layer"
        FA[FastAPI<br/>REST API Framework]
        UV[Uvicorn<br/>ASGI Server]
    end

    subgraph "Business Logic Layer"
        CLS_A[Classification Agent]
        CONS_A[Consistency Agent]
        RPT_A[Report Agent]
        SHARED[Shared Services]
    end

    subgraph "Task Queue Layer"
        CELERY[Celery<br/>Distributed Task Queue]
        REDIS_Q[Redis<br/>Message Broker]
    end

    subgraph "Data Access Layer"
        SQLA[SQLAlchemy<br/>ORM]
        FAISS_L[FAISS<br/>Vector Search]
        WHOOSH_L[Whoosh<br/>Full-text Search]
    end

    subgraph "Storage Layer"
        SQLITE[SQLite<br/>Relational DB]
        FS[File System<br/>Document Storage]
    end

    subgraph "External Services"
        AZURE[Azure OpenAI<br/>GPT-4 & Embeddings]
    end

    ST --> FA
    FA --> UV
    FA --> CLS_A
    FA --> CONS_A
    FA --> RPT_A
    
    CLS_A --> CELERY
    CONS_A --> CELERY
    RPT_A --> CELERY
    CLS_A --> SHARED
    CONS_A --> SHARED
    RPT_A --> SHARED
    
    CELERY --> REDIS_Q
    
    SHARED --> SQLA
    SHARED --> FAISS_L
    SHARED --> WHOOSH_L
    
    SQLA --> SQLITE
    FAISS_L --> FS
    WHOOSH_L --> FS
    
    CLS_A -.->|API Call| AZURE
    CONS_A -.->|API Call| AZURE
    RPT_A -.->|API Call| AZURE
```


## A1 Node ìƒì„¸ í”Œë¡œìš° (ì¡°í•­ ë§¤ì¹­)

```mermaid
flowchart TD
    START([A1 Node ì‹œì‘])
    
    subgraph "ì…ë ¥ ë°ì´í„° ì¤€ë¹„"
        LOAD_USER[ì‚¬ìš©ì ê³„ì•½ì„œ ë¡œë“œ<br/>parsed JSON]
        LOAD_INDEX[Ingestion ì¸ë±ìŠ¤ ë¡œë“œ<br/>ë¶„ë¥˜ëœ ìœ í˜•ì˜ í‘œì¤€ê³„ì•½ì„œ]
        PREP[ë§¤ì¹­ ëŒ€ìƒ ì¡°í•­ ì¤€ë¹„]
    end
    
    subgraph "1ì°¨ ë§¤ì¹­: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"
        LOOP_START{ëª¨ë“  ì‚¬ìš©ì<br/>ì¡°í•­ ì²˜ë¦¬?}
        GET_ARTICLE[ë‹¤ìŒ ì¡°í•­ ê°€ì ¸ì˜¤ê¸°]
        VECTOR[ë²¡í„° ê²€ìƒ‰<br/>FAISS ì¸ë±ìŠ¤ ì¿¼ë¦¬]
        KEYWORD[í‚¤ì›Œë“œ ê²€ìƒ‰<br/>Whoosh ì¸ë±ìŠ¤ ì¿¼ë¦¬]
        HYBRID[í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°<br/>ê°€ì¤‘ í‰ê· ]
        THRESHOLD{ìœ ì‚¬ë„ â‰¥<br/>ì„ê³„ê°’?}
        MATCH_FOUND[ë§¤ì¹­ ì„±ê³µ<br/>í‘œì¤€ ì¡°í•­ ì—°ê²°]
        NO_MATCH[ë§¤ì¹­ ì‹¤íŒ¨<br/>ëˆ„ë½ ëª©ë¡ ì¶”ê°€]
    end
    
    SAVE_JSON[article_mapping.json ì €ì¥<br/>matched + missing + extra]
    
    subgraph "ì¶œë ¥ ë°ì´í„° êµ¬ì¡°"
        OUTPUT["
        {
          matched: [
            {user_article, std_article, score}
          ],
          missing: [
            {user_article, reason}
          ],
          extra: [
            {std_article, reason}
          ]
        }
        "]
    end
    
    TRIGGER_A2A3[A2/A3 Node íŠ¸ë¦¬ê±°<br/>article_mapping.json ê¸°ë°˜]
    
    subgraph "ëˆ„ë½ ì¡°í•­ ì¬ê²€ì¦ (ë³‘ë ¬)"
        CHECK_MISSING{ëˆ„ë½ ì¡°í•­<br/>ì¡´ì¬?}
        LOOP_MISSING{ëª¨ë“  ëˆ„ë½<br/>ì¡°í•­ ì²˜ë¦¬?}
        GET_MISSING[ë‹¤ìŒ ëˆ„ë½ ì¡°í•­]
        LLM_VERIFY[LLM ì¬ê²€ì¦<br/>ë§¥ë½ ê¸°ë°˜ íŒë‹¨]
        LLM_DECISION{LLM íŒë‹¨<br/>ê²°ê³¼}
        REMAP[í‘œì¤€ ì¡°í•­ ì¬ë§¤ì¹­]
        CONFIRM_MISSING[ëˆ„ë½ í™•ì •]
        REVERIFY_DONE[ì¬ê²€ì¦ ì™„ë£Œ]
    end
    
    NOTE[A2/A3ëŠ” matchedë§Œ ì‚¬ìš©<br/>missing/extraëŠ” A1ì—ì„œë§Œ ì²˜ë¦¬]
    
    END([A1 Node ì™„ë£Œ])

    START --> LOAD_USER
    LOAD_USER --> LOAD_INDEX
    LOAD_INDEX --> PREP
    PREP --> LOOP_START
    
    LOOP_START -->|No| GET_ARTICLE
    GET_ARTICLE --> VECTOR
    GET_ARTICLE --> KEYWORD
    VECTOR --> HYBRID
    KEYWORD --> HYBRID
    HYBRID --> THRESHOLD
    
    THRESHOLD -->|Yes| MATCH_FOUND
    THRESHOLD -->|No| NO_MATCH
    
    MATCH_FOUND --> LOOP_START
    NO_MATCH --> LOOP_START
    
    LOOP_START -->|Yes| SAVE_JSON
    SAVE_JSON --> OUTPUT
    OUTPUT --> TRIGGER_A2A3
    TRIGGER_A2A3 --> NOTE
    
    SAVE_JSON --> CHECK_MISSING
    
    CHECK_MISSING -->|Yes| LOOP_MISSING
    CHECK_MISSING -->|No| END
    
    LOOP_MISSING -->|No| GET_MISSING
    GET_MISSING --> LLM_VERIFY
    LLM_VERIFY --> LLM_DECISION
    
    LLM_DECISION -->|ë§¤ì¹­ ê°€ëŠ¥| REMAP
    LLM_DECISION -->|ëˆ„ë½ í™•ì •| CONFIRM_MISSING
    
    REMAP --> LOOP_MISSING
    CONFIRM_MISSING --> LOOP_MISSING
    
    LOOP_MISSING -->|Yes| REVERIFY_DONE
    REVERIFY_DONE --> END
    
    NOTE --> EN

## A2/A3 Nodeì˜ A1 ê²°ê³¼ í™œìš©

```mermaid
flowchart LR
    subgraph "A1 ì¶œë ¥"
        A1_OUT[article_mapping.json<br/>matched + missing + extra]
    end
    
    subgraph "A2 Node: ë§¤ì¹­ëœ ì¡°í•­ë§Œ ì²˜ë¦¬"
        A2_READ[ë§¤ì¹­ ê²°ê³¼ ì½ê¸°]
        A2_FILTER[matched ì¡°í•­ë§Œ í•„í„°ë§<br/>missing/extra ë¬´ì‹œ]
        A2_LOAD[ì²´í¬ë¦¬ìŠ¤íŠ¸ ë¡œë“œ<br/>Ingestion ì¸ë±ìŠ¤]
        A2_CHECK[ë§¤ì¹­ëœ ì¡°í•­ ê¸°ì¤€<br/>ì²´í¬ë¦¬ìŠ¤íŠ¸ ê²€ì¦]
    end
    
    subgraph "A3 Node: ë§¤ì¹­ëœ ì¡°í•­ë§Œ ì²˜ë¦¬"
        A3_READ[ë§¤ì¹­ ê²°ê³¼ ì½ê¸°]
        A3_FILTER[matched ì¡°í•­ë§Œ í•„í„°ë§<br/>missing/extra ë¬´ì‹œ]
        A3_PAIR[ì¡°í•­ ìŒ ìƒì„±<br/>user â†” standard]
        A3_COMPARE[ìŒë³„ ë‚´ìš© ë¹„êµ<br/>ì˜ë¯¸ ì°¨ì´ ë¶„ì„]
    end
    
    A1_OUT --> A2_READ
    A1_OUT --> A3_READ
    
    A2_READ --> A2_FILTER
    A2_FILTER --> A2_LOAD
    A2_LOAD --> A2_CHECK
    
    A3_READ --> A3_FILTER
    A3_FILTER --> A3_PAIR
    A3_PAIR --> A3_COMPARE
    
    style A2_FILTER fill:#e1f5ff
    style A3_FILTER fill:#e1f5ff
```

## ë°ì´í„° íë¦„: A1 â†’ A2/A3

```mermaid
sequenceDiagram
    participant A1 as A1 Node
    participant FS as File System
    participant A2 as A2 Node
    participant A3 as A3 Node
    participant DB as Database

    Note over A1: ì¡°í•­ ë§¤ì¹­ ìˆ˜í–‰
    A1->>A1: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    A1->>FS: article_mapping.json ì €ì¥
    
    Note over A1,A3: JSON ì €ì¥ ì¦‰ì‹œ A2/A3 ì‹œì‘
    
    par A2 Node ì‹¤í–‰
        A2->>FS: article_mapping.json ë¡œë“œ
        A2->>A2: matched ì¡°í•­ë§Œ í•„í„°ë§
        A2->>A2: ì²´í¬ë¦¬ìŠ¤íŠ¸ ê²€ì¦
        A2->>FS: checklist_result.json ì €ì¥
    and A3 Node ì‹¤í–‰
        A3->>FS: article_mapping.json ë¡œë“œ
        A3->>A3: matched ì¡°í•­ë§Œ í•„í„°ë§
        A3->>A3: ë‚´ìš© ë¹„êµ ë¶„ì„
        A3->>FS: content_comparison.json ì €ì¥
    and A1 ì¬ê²€ì¦ (ë³‘ë ¬)
        A1->>A1: ëˆ„ë½ ì¡°í•­ ì¬ê²€ì¦ (LLM)
        A1->>A1: ì¬ê²€ì¦ ê²°ê³¼ ë°˜ì˜
    end
    
    Note over A1,A3: ëª¨ë“  ë…¸ë“œ ì™„ë£Œ, ê²°ê³¼ í†µí•©
    
    A1->>DB: ë§¤ì¹­ ê²°ê³¼ ì €ì¥
    A2->>DB: ê²€ì¦ ê²°ê³¼ ì €ì¥
    A3->>DB: ë¹„êµ ê²°ê³¼ ì €ì¥
```


## Ingestion Pipeline ìƒì„¸ ì•„í‚¤í…ì²˜

### Ingestion ì „ì²´ êµ¬ì¡°

```mermaid
flowchart TB
    subgraph "ì…ë ¥ ë¬¸ì„œ"
        STD_PDF[í‘œì¤€ê³„ì•½ì„œ<br/>PDF]
        STD_DOCX[í‘œì¤€ê³„ì•½ì„œ<br/>DOCX]
        GUIDE_PDF[í™œìš©ì•ˆë‚´ì„œ<br/>PDF]
        GUIDE_DOCX[í™œìš©ì•ˆë‚´ì„œ<br/>DOCX]
    end

    subgraph "1ë‹¨ê³„: íŒŒì‹± (Parsers)"
        STD_PDF_PARSER[StdContractPdfParser]
        STD_DOCX_PARSER[StdContractDocxParser]
        GUIDE_PDF_PARSER[GuidebookPdfParser]
        GUIDE_DOCX_PARSER[GuidebookDocxParser]
    end

    subgraph "ì¤‘ê°„ ë°ì´í„°"
        STRUCTURED[êµ¬ì¡°í™” ë°ì´í„°<br/>*_structured.json]
    end

    subgraph "2ë‹¨ê³„: ì²­í‚¹ (Processors)"
        ART_CHUNKER[ArticleChunker<br/>ì¡°/ë³„ì§€ ë‹¨ìœ„]
        CLAUSE_CHUNKER[ClauseChunker<br/>í•­/í˜¸ ë‹¨ìœ„]
    end

    subgraph "ì²­í¬ ë°ì´í„°"
        ART_CHUNKS[ì¡° ë‹¨ìœ„ ì²­í¬<br/>*_art_chunks.json]
        CLAUSE_CHUNKS[í•­/í˜¸ ì²­í¬<br/>*_chunks.json]
    end

    subgraph "3ë‹¨ê³„: ì„ë² ë”© (Processors)"
        EMBEDDER[TextEmbedder<br/>Azure OpenAI]
    end

    subgraph "4ë‹¨ê³„: ì¸ë±ì‹± (Indexers)"
        FAISS_IDX[FAISS Indexer<br/>ë²¡í„° ê²€ìƒ‰]
        WHOOSH_IDX[Whoosh Indexer<br/>í‚¤ì›Œë“œ ê²€ìƒ‰]
    end

    subgraph "ê²€ìƒ‰ ì¸ë±ìŠ¤"
        FAISS_INDEX[FAISS Index<br/>*.faiss + *.pkl]
        WHOOSH_INDEX[Whoosh Index<br/>schema + segments]
    end

    STD_PDF --> STD_PDF_PARSER
    STD_DOCX --> STD_DOCX_PARSER
    GUIDE_PDF --> GUIDE_PDF_PARSER
    GUIDE_DOCX --> GUIDE_DOCX_PARSER

    STD_PDF_PARSER --> STRUCTURED
    STD_DOCX_PARSER --> STRUCTURED
    GUIDE_PDF_PARSER --> STRUCTURED
    GUIDE_DOCX_PARSER --> STRUCTURED

    STRUCTURED --> ART_CHUNKER
    STRUCTURED --> CLAUSE_CHUNKER

    ART_CHUNKER --> ART_CHUNKS
    CLAUSE_CHUNKER --> CLAUSE_CHUNKS

    CLAUSE_CHUNKS --> EMBEDDER

    EMBEDDER --> FAISS_IDX
    EMBEDDER --> WHOOSH_IDX

    FAISS_IDX --> FAISS_INDEX
    WHOOSH_IDX --> WHOOSH_INDEX
```

### Ingestion íŒŒì´í”„ë¼ì¸ í”Œë¡œìš°

```mermaid
sequenceDiagram
    participant CLI as Ingestion CLI
    participant Parser as Parser Module
    participant Chunker as Chunker Module
    participant Embedder as Embedder Module
    participant FAISS as FAISS Indexer
    participant Whoosh as Whoosh Indexer
    participant FS as File System
    participant Azure as Azure OpenAI

    Note over CLI: run --mode full --file all

    CLI->>Parser: 1. íŒŒì‹± ì‹œì‘
    Parser->>FS: ì›ë³¸ ë¬¸ì„œ ì½ê¸°<br/>source_documents/
    Parser->>Parser: ë¬¸ì„œ êµ¬ì¡° ë¶„ì„<br/>(ì¡°, í•­, í˜¸ ì¶”ì¶œ)
    Parser->>FS: êµ¬ì¡°í™” ë°ì´í„° ì €ì¥<br/>extracted_documents/<br/>*_structured.json

    CLI->>Chunker: 2. ì²­í‚¹ ì‹œì‘
    Chunker->>FS: êµ¬ì¡°í™” ë°ì´í„° ì½ê¸°
    Chunker->>Chunker: í•­/í˜¸ ë‹¨ìœ„ ë¶„í• <br/>ë©”íƒ€ë°ì´í„° ìƒì„±
    Chunker->>FS: ì²­í¬ ë°ì´í„° ì €ì¥<br/>chunked_documents/<br/>*_chunks.json

    CLI->>Embedder: 3. ì„ë² ë”© ì‹œì‘
    Embedder->>FS: ì²­í¬ ë°ì´í„° ì½ê¸°
    
    loop ê° ì²­í¬
        Embedder->>Azure: ì„ë² ë”© ìš”ì²­<br/>text-embedding-3-large
        Azure-->>Embedder: ë²¡í„° ë°˜í™˜ (3072ì°¨ì›)
    end

    Embedder->>FAISS: 4a. FAISS ì¸ë±ì‹±
    FAISS->>FAISS: ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±
    FAISS->>FS: ì¸ë±ìŠ¤ ì €ì¥<br/>search_indexes/faiss/<br/>*.faiss + *.pkl

    Embedder->>Whoosh: 4b. Whoosh ì¸ë±ì‹±
    Whoosh->>Whoosh: í‚¤ì›Œë“œ ì¸ë±ìŠ¤ ìƒì„±<br/>BM25 ì•Œê³ ë¦¬ì¦˜
    Whoosh->>FS: ì¸ë±ìŠ¤ ì €ì¥<br/>search_indexes/whoosh/<br/>schema + segments

    Note over CLI: ì¸ë±ì‹± ì™„ë£Œ
```

### íŒŒì„œ ëª¨ë“ˆ êµ¬ì¡°

```mermaid
flowchart TD
    INPUT[ì›ë³¸ ë¬¸ì„œ]
    
    subgraph "íŒŒì„œ ì„ íƒ ë¡œì§"
        CHECK_EXT{íŒŒì¼ í™•ì¥ì?}
        CHECK_TYPE{ë¬¸ì„œ ìœ í˜•?}
    end
    
    subgraph "í‘œì¤€ê³„ì•½ì„œ íŒŒì„œ"
        STD_PDF[StdContractPdfParser<br/>PyMuPDF ê¸°ë°˜]
        STD_DOCX[StdContractDocxParser<br/>python-docx ê¸°ë°˜]
    end
    
    subgraph "í™œìš©ì•ˆë‚´ì„œ íŒŒì„œ"
        GUIDE_PDF[GuidebookPdfParser<br/>PyMuPDF ê¸°ë°˜]
        GUIDE_DOCX[GuidebookDocxParser<br/>python-docx ê¸°ë°˜]
    end
    
    subgraph "íŒŒì‹± ê²°ê³¼"
        STRUCTURE["
        {
          contract_type: string,
          title: string,
          articles: [
            {
              article_num: string,
              article_title: string,
              clauses: [
                {
                  clause_num: string,
                  content: string,
                  items: [...]
                }
              ]
            }
          ]
        }
        "]
    end
    
    OUTPUT[*_structured.json]

    INPUT --> CHECK_EXT
    CHECK_EXT -->|.pdf| CHECK_TYPE
    CHECK_EXT -->|.docx| CHECK_TYPE
    
    CHECK_TYPE -->|í‘œì¤€ê³„ì•½ì„œ| STD_PDF
    CHECK_TYPE -->|í‘œì¤€ê³„ì•½ì„œ| STD_DOCX
    CHECK_TYPE -->|í™œìš©ì•ˆë‚´ì„œ| GUIDE_PDF
    CHECK_TYPE -->|í™œìš©ì•ˆë‚´ì„œ| GUIDE_DOCX
    
    STD_PDF --> STRUCTURE
    STD_DOCX --> STRUCTURE
    GUIDE_PDF --> STRUCTURE
    GUIDE_DOCX --> STRUCTURE
    
    STRUCTURE --> OUTPUT
```

### ì²­ì»¤ ëª¨ë“ˆ êµ¬ì¡°

```mermaid
flowchart TD
    INPUT[*_structured.json]
    
    subgraph "ì²­ì»¤ ì„ íƒ"
        CHOICE{ì²­í‚¹ ë‹¨ìœ„?}
    end
    
    subgraph "ArticleChunker (ì¡° ë‹¨ìœ„)"
        ART_LOAD[êµ¬ì¡°í™” ë°ì´í„° ë¡œë“œ]
        ART_SPLIT[ì¡°/ë³„ì§€ ë‹¨ìœ„ ë¶„í• ]
        ART_META[ë©”íƒ€ë°ì´í„° ìƒì„±<br/>contract_type, article_num]
        ART_OUT[*_art_chunks.json]
    end
    
    subgraph "ClauseChunker (í•­/í˜¸ ë‹¨ìœ„)"
        CLS_LOAD[êµ¬ì¡°í™” ë°ì´í„° ë¡œë“œ]
        CLS_SPLIT[í•­/í˜¸ ë‹¨ìœ„ ë¶„í• ]
        CLS_META[ë©”íƒ€ë°ì´í„° ìƒì„±<br/>article_num, clause_num, item_num]
        CLS_OUT[*_chunks.json]
    end
    
    subgraph "ì²­í¬ êµ¬ì¡°"
        CHUNK["
        {
          chunk_id: string,
          text: string,
          metadata: {
            contract_type: string,
            article_num: string,
            article_title: string,
            clause_num: string,
            item_num: string
          }
        }
        "]
    end

    INPUT --> CHOICE
    
    CHOICE -->|ì¡° ë‹¨ìœ„| ART_LOAD
    ART_LOAD --> ART_SPLIT
    ART_SPLIT --> ART_META
    ART_META --> CHUNK
    CHUNK --> ART_OUT
    
    CHOICE -->|í•­/í˜¸ ë‹¨ìœ„| CLS_LOAD
    CLS_LOAD --> CLS_SPLIT
    CLS_SPLIT --> CLS_META
    CLS_META --> CHUNK
    CHUNK --> CLS_OUT
```

### ì„ë² ë”© ë° ì¸ë±ì‹± í”Œë¡œìš°

```mermaid
flowchart TD
    INPUT[*_chunks.json]
    
    subgraph "TextEmbedder"
        LOAD[ì²­í¬ ë°ì´í„° ë¡œë“œ]
        BATCH[ë°°ì¹˜ ì²˜ë¦¬<br/>ì²­í¬ ê·¸ë£¹í™”]
    end
    
    subgraph "FAISS ê²½ë¡œ: ë²¡í„° ì¸ë±ì‹±"
        subgraph "ì„ë² ë”© ìƒì„±"
            AZURE[Azure OpenAI API<br/>text-embedding-3-large]
            VECTOR[ë²¡í„° ìƒì„±<br/>3072ì°¨ì›]
        end
        
        COLLECT[ë²¡í„° ìˆ˜ì§‘]
        
        FAISS_BUILD[FAISS ì¸ë±ìŠ¤ ìƒì„±<br/>IndexFlatIP]
        FAISS_ADD[ë²¡í„° ì¶”ê°€]
        FAISS_SAVE[ì¸ë±ìŠ¤ ì €ì¥<br/>*.faiss]
        FAISS_META[ë©”íƒ€ë°ì´í„° ì €ì¥<br/>*.pkl]
    end
    
    subgraph "Whoosh ê²½ë¡œ: í‚¤ì›Œë“œ ì¸ë±ì‹±"
        WHOOSH_SCHEMA[ìŠ¤í‚¤ë§ˆ ì •ì˜<br/>TEXT, ID, STORED]
        WHOOSH_BUILD[ì¸ë±ìŠ¤ ìƒì„±<br/>BM25 ì•Œê³ ë¦¬ì¦˜]
        WHOOSH_ADD[í…ìŠ¤íŠ¸ ë¬¸ì„œ ì¶”ê°€<br/>ì²­í¬ í…ìŠ¤íŠ¸ ì§ì ‘ ì‚¬ìš©]
        WHOOSH_COMMIT[ì¸ë±ìŠ¤ ì»¤ë°‹]
    end
    
    OUTPUT_FAISS[search_indexes/faiss/<br/>*.faiss + *.pkl]
    OUTPUT_WHOOSH[search_indexes/whoosh/<br/>schema + segments]

    INPUT --> LOAD
    LOAD --> BATCH
    
    BATCH --> AZURE
    AZURE --> VECTOR
    VECTOR --> COLLECT
    COLLECT --> FAISS_BUILD
    FAISS_BUILD --> FAISS_ADD
    FAISS_ADD --> FAISS_SAVE
    FAISS_SAVE --> FAISS_META
    FAISS_META --> OUTPUT_FAISS
    
    BATCH --> WHOOSH_SCHEMA
    WHOOSH_SCHEMA --> WHOOSH_BUILD
    WHOOSH_BUILD --> WHOOSH_ADD
    WHOOSH_ADD --> WHOOSH_COMMIT
    WHOOSH_COMMIT --> OUTPUT_WHOOSH
```
=======
    participant UI as Streamlit
    participant API as FastAPI
    participant Redis as Redis Queue
    participant CW1 as Classification<br/>Worker
    participant CW2 as Consistency<br/>Worker
    participant DB as SQLite
    participant Search as FAISS + Whoosh
    participant LLM as Azure OpenAI

    %% ì—…ë¡œë“œ ë‹¨ê³„
    User->>UI: 1. DOCX íŒŒì¼ ì—…ë¡œë“œ
    UI->>API: POST /upload (multipart)
    API->>API: UserContractParser íŒŒì‹±
    API->>LLM: ì„ë² ë”© ìƒì„± (ê° ì¡°ë¬¸)
    API->>DB: ContractDocument ì €ì¥<br/>(parsed_data + embeddings)
    API->>Redis: classify_contract_task ë°œí–‰
    API-->>UI: contract_id ë°˜í™˜

    %% ë¶„ë¥˜ ë‹¨ê³„
    Redis->>CW1: Task ë°°í¬
    CW1->>DB: ê³„ì•½ì„œ ì¡°íšŒ
    CW1->>Search: 5ê°œ í‘œì¤€ê³„ì•½ì„œ ìœ ì‚¬ë„ ê³„ì‚°
    alt Gap >= 0.05 (ëª…í™•í•œ ê²½ìš°)
        CW1->>CW1: ì„ë² ë”© ê²°ê³¼ ì‚¬ìš© (LLM ìƒëµ)
    else Gap < 0.05 (ì• ë§¤í•œ ê²½ìš°)
        CW1->>LLM: Few-shot ë¶„ë¥˜ (5ê°œ ì˜ˆì œ)
    end
    CW1->>DB: ClassificationResult ì €ì¥

    UI->>API: 2. í´ë§: GET /api/classification/:id
    API-->>UI: ë¶„ë¥˜ ê²°ê³¼ (5ê°œ ìœ í˜• ì ìˆ˜)

    User->>UI: 3. ë¶„ë¥˜ í™•ì¸/ìˆ˜ì •
    UI->>API: POST /api/classification/:id/confirm

    %% ê²€ì¦ ë‹¨ê³„
    User->>UI: 4. "ê³„ì•½ì„œ ê²€ì¦" ë²„íŠ¼ í´ë¦­
    UI->>API: POST /api/validation/:id/start<br/>(text_weight, title_weight, dense_weight)
    API->>Redis: validate_contract_task ë°œí–‰
    API-->>UI: task_id ë°˜í™˜

    %% A1: ì™„ì „ì„± ê²€ì‚¬
    Redis->>CW2: Task ë°°í¬
    Note over CW2: A1 Node: Completeness Check
    CW2->>DB: í‘œì¤€ ê³„ì•½ì„œ ì²­í¬ ì¡°íšŒ
    loop ê° ì‚¬ìš©ì ì¡°ë¬¸
        CW2->>Search: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (FAISS + Whoosh)
        CW2->>CW2: ì¡°ë¬¸ ë‹¨ìœ„ ì ìˆ˜ ì§‘ê³„
    end
    CW2->>CW2: ëˆ„ë½ëœ í‘œì¤€ ì¡°ë¬¸ ì‹ë³„
    CW2->>LLM: ëˆ„ë½ ì¡°ë¬¸ ì¬ê²€ì¦ (ê±°ì§“ ì–‘ì„± ì œê±°)
    CW2->>DB: ValidationResult.completeness_check ì €ì¥

    %% A2: ì²´í¬ë¦¬ìŠ¤íŠ¸ ê²€ì¦
    Note over CW2: A2 Node: Checklist Check
    CW2->>CW2: ë§¤ì¹­ëœ ì¡°ë¬¸ë³„ ì²´í¬ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
    loop ê° ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª©
        CW2->>LLM: ìë™ ê²€ì¦ (YES/NO/UNCLEAR/MANUAL)
    end
    CW2->>DB: ValidationResult.checklist_validation ì €ì¥

    %% A3: ë‚´ìš© ë¶„ì„
    Note over CW2: A3 Node: Content Analysis
    loop ê° ë§¤ì¹­ëœ ì¡°ë¬¸
        CW2->>Search: í‘œì¤€ ë‚´ìš© ê²€ìƒ‰ (ì„¸ë¶€ í•­ëª© ë‹¨ìœ„)
        CW2->>LLM: ë‚´ìš© ë¹„êµ ë° ê°œì„ ì•ˆ ìƒì„±
    end
    CW2->>DB: ValidationResult.content_analysis ì €ì¥

    UI->>API: 5. í´ë§: GET /api/validation/:id
    API-->>UI: ê²€ì¦ ê²°ê³¼ (A1+A2+A3)
    UI->>User: 6. ê²°ê³¼ ì‹œê°í™”<br/>(ë§¤ì¹­, ì²´í¬ë¦¬ìŠ¤íŠ¸, ë¶„ì„, ì œì•ˆ)
```

### 2. ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶• í”Œë¡œìš°

```mermaid
graph LR
    A[í‘œì¤€ ê³„ì•½ì„œ PDF/DOCX<br/>5ê°œ íŒŒì¼] --> B[Parsing<br/>ì¡°ë¬¸ êµ¬ì¡° ì¶”ì¶œ]
    B --> C[Chunking<br/>ì¡°ë¬¸ ë‹¨ìœ„ ì²­í‚¹]
    C --> D[Embedding<br/>Azure OpenAI<br/>text-embedding-3-large]
    D --> E1[FAISS ì¸ë±ì‹±<br/>text_norm + title<br/>ì´ì¤‘ ì¸ë±ìŠ¤]
    D --> E2[Whoosh ì¸ë±ì‹±<br/>BM25 + Mecab<br/>í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„]

    style A fill:#ffccbc
    style B fill:#fff9c4
    style C fill:#c5e1a5
    style D fill:#b3e5fc
    style E1 fill:#e1bee7
    style E2 fill:#f8bbd0
```

**ì‹¤í–‰ ëª…ë ¹**:
```bash
docker-compose --profile ingestion run --rm ingestion run --mode full --file all
```

---

## ê²€ìƒ‰ ì•„í‚¤í…ì²˜
>>>>>>> c7c7d8f082d58f15a66cf160f5d601f1ab908b93

### í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ êµ¬ì¡°

```mermaid
<<<<<<< HEAD
flowchart TD
    QUERY[ì‚¬ìš©ì ì¿¼ë¦¬]
    
    subgraph "HybridSearcher"
        LOAD_IDX[ì¸ë±ìŠ¤ ë¡œë“œ<br/>FAISS + Whoosh]
        
        subgraph "Dense ê²€ìƒ‰ (FAISS)"
            EMBED_Q[ì¿¼ë¦¬ ì„ë² ë”©<br/>Azure OpenAI]
            FAISS_SEARCH[ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰<br/>ì½”ì‚¬ì¸ ìœ ì‚¬ë„]
            DENSE_RESULTS[Dense ê²°ê³¼<br/>+ ì ìˆ˜]
        end
        
        subgraph "Sparse ê²€ìƒ‰ (Whoosh)"
            WHOOSH_SEARCH[í‚¤ì›Œë“œ ê²€ìƒ‰<br/>BM25 ì•Œê³ ë¦¬ì¦˜]
            SPARSE_RESULTS[Sparse ê²°ê³¼<br/>+ ì ìˆ˜]
        end
        
        subgraph "ì ìˆ˜ í†µí•©"
            NORMALIZE[ì ìˆ˜ ì •ê·œí™”<br/>0~1 ë²”ìœ„]
            WEIGHTED[ê°€ì¤‘ í‰ê· <br/>dense_weight * dense_score<br/>+ sparse_weight * sparse_score]
            RERANK[ì¬ìˆœìœ„í™”<br/>í†µí•© ì ìˆ˜ ê¸°ì¤€]
        end
        
        TOP_K[Top-K ê²°ê³¼ ë°˜í™˜]
    end
    
    OUTPUT[ê²€ìƒ‰ ê²°ê³¼<br/>+ ë©”íƒ€ë°ì´í„°]

    QUERY --> LOAD_IDX
    LOAD_IDX --> EMBED_Q
    LOAD_IDX --> WHOOSH_SEARCH
    
    EMBED_Q --> FAISS_SEARCH
    FAISS_SEARCH --> DENSE_RESULTS
    
    WHOOSH_SEARCH --> SPARSE_RESULTS
    
    DENSE_RESULTS --> NORMALIZE
    SPARSE_RESULTS --> NORMALIZE
    
    NORMALIZE --> WEIGHTED
    WEIGHTED --> RERANK
    RERANK --> TOP_K
    TOP_K --> OUTPUT
```

### Ingestion ë””ë ‰í† ë¦¬ êµ¬ì¡°

```mermaid
graph TB
    subgraph "ingestion/"
        INGEST[ingest.py<br/>CLI ë©”ì¸ ëª¨ë“ˆ]
        
        subgraph "parsers/"
            STD_PDF_P[std_contract_pdf_parser.py]
            STD_DOCX_P[std_contract_docx_parser.py]
            GUIDE_PDF_P[guidebook_pdf_parser.py]
            GUIDE_DOCX_P[guidebook_docx_parser.py]
        end
        
        subgraph "processors/"
            ART_C[art_chunker.py<br/>ì¡° ë‹¨ìœ„ ì²­í‚¹]
            CLAUSE_C[chunker.py<br/>í•­/í˜¸ ë‹¨ìœ„ ì²­í‚¹]
            EMBED[embedder.py<br/>ì„ë² ë”© ìƒì„±]
            SEARCH[searcher.py<br/>í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰]
            S_EMBED[s_embedder.py<br/>ê°„ì´ ì„ë² ë”©]
            S_SEARCH[s_searcher.py<br/>ê°„ì´ ê²€ìƒ‰]
        end
        
        subgraph "indexers/"
            FAISS_I[faiss_indexer.py<br/>ë²¡í„° ì¸ë±ì‹±]
            WHOOSH_I[whoosh_indexer.py<br/>í‚¤ì›Œë“œ ì¸ë±ì‹±]
        end
    end
    
    INGEST --> STD_PDF_P
    INGEST --> STD_DOCX_P
    INGEST --> GUIDE_PDF_P
    INGEST --> GUIDE_DOCX_P
    
    INGEST --> ART_C
    INGEST --> CLAUSE_C
    INGEST --> EMBED
    INGEST --> SEARCH
    
    EMBED --> FAISS_I
    EMBED --> WHOOSH_I
```

### Ingestion ì‹¤í–‰ ëª¨ë“œ

```mermaid
flowchart LR
    subgraph "ì‹¤í–‰ ëª¨ë“œ"
        FULL[full<br/>ì „ì²´ íŒŒì´í”„ë¼ì¸]
        PARSING[parsing<br/>íŒŒì‹±ë§Œ]
        ART_CHUNK[art_chunking<br/>ì¡° ë‹¨ìœ„ ì²­í‚¹]
        CHUNK[chunking<br/>í•­/í˜¸ ì²­í‚¹]
        EMBED[embedding<br/>ì„ë² ë”©+ì¸ë±ì‹±]
        S_EMBED[s_embedding<br/>ê°„ì´ ì„ë² ë”©]
    end
    
    subgraph "íŒŒì´í”„ë¼ì¸ ë‹¨ê³„"
        P[íŒŒì‹±]
        C[ì²­í‚¹]
        E[ì„ë² ë”©]
        I[ì¸ë±ì‹±]
    end
    
    FULL --> P
    P --> C
    C --> E
    E --> I
    
    PARSING --> P
    ART_CHUNK --> C
    CHUNK --> C
    EMBED --> E
    EMBED --> I
    S_EMBED --> E
    S_EMBED --> I
```

### ë°ì´í„° íë¦„: Ingestion â†’ Agents

```mermaid
flowchart LR
    subgraph "Ingestion ì¶œë ¥"
        FAISS[FAISS Index<br/>ë²¡í„° ê²€ìƒ‰]
        WHOOSH[Whoosh Index<br/>í‚¤ì›Œë“œ ê²€ìƒ‰]
        CHUNKS[Chunk Metadata<br/>*.pkl]
    end
    
    subgraph "Agent ì‚¬ìš©"
        CLS[Classification Agent<br/>í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰]
        A1[Consistency A1 Node<br/>ì¡°í•­ ë§¤ì¹­]
        A2[Consistency A2 Node<br/>ì²´í¬ë¦¬ìŠ¤íŠ¸ ê²€ì¦]
    end
    
    FAISS --> CLS
    WHOOSH --> CLS
    CHUNKS --> CLS
    
    FAISS --> A1
    WHOOSH --> A1
    CHUNKS --> A1
    
    FAISS --> A2
    WHOOSH --> A2
    CHUNKS --> A2
```
=======
graph TB
    subgraph "Query Input"
        Q[ê²€ìƒ‰ ì¿¼ë¦¬<br/>text: ì¡°ë¬¸ ë‚´ìš©<br/>title: ì¡°ë¬¸ ì œëª©]
    end

    subgraph "Dual Vector Search FAISS"
        E[Embedding Generator<br/>Azure OpenAI]
        F1[Text Index<br/>text_norm embedding<br/>~400 chunks]
        F2[Title Index<br/>title embedding<br/>~400 chunks]
        E --> F1
        E --> F2
        F1 --> DS1[Top 50<br/>text score]
        F2 --> DS2[Top 50<br/>title score]
    end

    subgraph "Keyword Search Whoosh"
        W1[BM25 Scorer<br/>Mecab Tokenizer]
        W1 --> WS1[Top 50<br/>text score]
        W1 --> WS2[Top 50<br/>title score]
    end

    subgraph "Score Fusion"
        N1[Min-Max Normalization]
        N2[Weighted Fusion]
        DS1 --> N1
        DS2 --> N1
        WS1 --> N1
        WS2 --> N1
        N1 --> N2
    end

    subgraph "Final Ranking"
        R[Top K Results<br/>global_id, score, reasoning]
    end

    Q --> E
    Q --> W1
    N2 --> R

    style Q fill:#e3f2fd
    style E fill:#fff9c4
    style F1 fill:#f3e5f5
    style F2 fill:#f3e5f5
    style W1 fill:#fce4ec
    style N2 fill:#c8e6c9
    style R fill:#ffccbc
```

### ê°€ì¤‘ì¹˜ êµ¬ì¡°

| ë ˆë²¨ | íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… |
|-----|----------|--------|------|
| **í•„ë“œ ê°€ì¤‘ì¹˜** | text_weight | 0.7 | ì¡°ë¬¸ ë‚´ìš© ì¤‘ìš”ë„ |
| | title_weight | 0.3 | ì¡°ë¬¸ ì œëª© ì¤‘ìš”ë„ |
| **ê²€ìƒ‰ ë°©ì‹ ê°€ì¤‘ì¹˜** | dense_weight | 0.85 | ë²¡í„° ê²€ìƒ‰ ë¹„ì¤‘ |
| | sparse_weight | 0.15 | í‚¤ì›Œë“œ ê²€ìƒ‰ ë¹„ì¤‘ |

**ìˆ˜ì‹**:
```
final_score = (text_score * 0.7 + title_score * 0.3) * 0.85(dense) +
              (text_score * 0.7 + title_score * 0.3) * 0.15(sparse)
```

**ì ì‘í˜• ê°€ì¤‘ì¹˜**:
- Sparse ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ â†’ Dense ê°€ì¤‘ì¹˜ 1.0ìœ¼ë¡œ ìë™ ì¡°ì • (0.85 ì œí•œ í•´ì œ)

---

## ë°ì´í„° ì €ì¥ì†Œ

### 1. SQLite Database ìŠ¤í‚¤ë§ˆ

```mermaid
erDiagram
    ContractDocument ||--o{ ClassificationResult : "has"
    ContractDocument ||--o{ ValidationResult : "has"
    ContractDocument ||--o{ Report : "has"
    ContractDocument ||--o{ TokenUsage : "tracks"

    ContractDocument {
        string contract_id PK
        string filename
        string file_path
        datetime upload_date
        json parsed_data "êµ¬ì¡°í™”ëœ ì¡°ë¬¸ + ì„ë² ë”©"
        json parsed_metadata "íŒŒì‹± í†µê³„"
        string status "uploaded|parsing|parsed|classifying|classified|validating|validated|error"
    }

    ClassificationResult {
        int id PK
        string contract_id FK
        string predicted_type "provide|create|process|brokerage_provider|brokerage_user"
        float confidence
        json scores "5ê°œ ìœ í˜•ë³„ ì ìˆ˜"
        string confirmed_type "ì‚¬ìš©ì í™•ì¸/ìˆ˜ì •"
        boolean user_override
        string reasoning
        datetime created_at
    }

    ValidationResult {
        int id PK
        string contract_id FK
        string contract_type
        json completeness_check "A1 ë§¤ì¹­ ê²°ê³¼"
        json checklist_validation "A2 ì²´í¬ë¦¬ìŠ¤íŠ¸ ê²°ê³¼"
        json content_analysis "A3 ë‚´ìš© ë¶„ì„"
        float overall_score
        json recommendations
        datetime created_at
    }

    Report {
        int id PK
        string contract_id FK
        string contract_type
        text overall_assessment
        json issues
        json positive_points
        json recommendations
        datetime created_at
    }

    TokenUsage {
        int id PK
        string contract_id FK
        string component "classification_agent|consistency_agent"
        string api_type "chat_completion|embedding"
        string model
        int prompt_tokens
        int completion_tokens
        int total_tokens
        json extra_info
        datetime timestamp
    }
```

### 2. íŒŒì¼ ì‹œìŠ¤í…œ êµ¬ì¡°

```
c:\Python Projects\data-contract-project\
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â””â”€â”€ contracts.db                              # SQLite DB
â”‚   â”œâ”€â”€ source_documents/                             # í‘œì¤€ ê³„ì•½ì„œ ì›ë³¸ (5ê°œ)
â”‚   â”‚   â”œâ”€â”€ provide_std_contract.pdf
â”‚   â”‚   â”œâ”€â”€ create_std_contract.pdf
â”‚   â”‚   â”œâ”€â”€ process_std_contract.pdf
â”‚   â”‚   â”œâ”€â”€ brokerage_provider_std_contract.pdf
â”‚   â”‚   â””â”€â”€ brokerage_user_std_contract.pdf
â”‚   â”œâ”€â”€ extracted_documents/                          # íŒŒì‹±ëœ JSON (5ê°œ)
â”‚   â”‚   â””â”€â”€ {type}_std_contract_structured.json      # ì¡°ë¬¸ êµ¬ì¡°
â”‚   â”œâ”€â”€ chunked_documents/                            # ì²­í¬ JSON (5ê°œ)
â”‚   â”‚   â””â”€â”€ {type}_std_contract_chunks.json          # ~80-100 ì¡°ë¬¸/ì²­í¬
â”‚   â”œâ”€â”€ parsed_user_contracts/                        # ì‚¬ìš©ì ê³„ì•½ì„œ (ë””ë²„ê¹…ìš©)
â”‚   â”‚   â””â”€â”€ {filename}_{contract_id}.json
â”‚   â””â”€â”€ sample_user_contracts/                        # í…ŒìŠ¤íŠ¸ íŒŒì¼
â”‚
â””â”€â”€ search_indexes/
    â”œâ”€â”€ faiss/                                        # ë²¡í„° ì¸ë±ìŠ¤ (10ê°œ)
    â”‚   â”œâ”€â”€ provide_std_contract_text.faiss          # ë‚´ìš© ì¸ë±ìŠ¤
    â”‚   â”œâ”€â”€ provide_std_contract_title.faiss         # ì œëª© ì¸ë±ìŠ¤
    â”‚   â”œâ”€â”€ create_std_contract_text.faiss
    â”‚   â”œâ”€â”€ create_std_contract_title.faiss
    â”‚   â”œâ”€â”€ process_std_contract_text.faiss
    â”‚   â”œâ”€â”€ process_std_contract_title.faiss
    â”‚   â”œâ”€â”€ brokerage_provider_std_contract_text.faiss
    â”‚   â”œâ”€â”€ brokerage_provider_std_contract_title.faiss
    â”‚   â”œâ”€â”€ brokerage_user_std_contract_text.faiss
    â”‚   â””â”€â”€ brokerage_user_std_contract_title.faiss
    â”‚
    â””â”€â”€ whoosh/                                       # í‚¤ì›Œë“œ ì¸ë±ìŠ¤ (5ê°œ)
        â”œâ”€â”€ provide_std_contract/
        â”‚   â”œâ”€â”€ _MAIN_*.toc
        â”‚   â””â”€â”€ _MAIN_*.seg
        â”œâ”€â”€ create_std_contract/
        â”œâ”€â”€ process_std_contract/
        â”œâ”€â”€ brokerage_provider_std_contract/
        â””â”€â”€ brokerage_user_std_contract/
```

### 3. ì²­í¬ ë°ì´í„° êµ¬ì¡°

```json
{
  "id": "chunk_001",
  "global_id": "urn:contract:provide:article:1",
  "unit_type": "article",
  "parent_id": null,
  "title": "ì œ1ì¡°(ëª©ì )",
  "text_raw": "ì´ ê³„ì•½ì€ ë°ì´í„° ì œê³µì— ê´€í•œ...",
  "text_norm": "ê³„ì•½ ë°ì´í„° ì œê³µ ...",
  "source_file": "provide_std_contract",
  "order_index": 1,
  "embeddings": {
    "title": [0.012, -0.045, ...],      // 3072 dim
    "text_norm": [0.023, -0.012, ...]   // 3072 dim
  }
}
```

---

## AI/ML í†µí•©

### Azure OpenAI ì‚¬ìš© í˜„í™©

```mermaid
graph TB
    subgraph "Embedding API"
        E1[ì‚¬ìš©ì ê³„ì•½ì„œ ì—…ë¡œë“œ<br/>FastAPI /upload]
        E2[ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶•<br/>Ingestion CLI]
        E3[A3 ë‚´ìš© ê²€ìƒ‰<br/>Consistency Worker]
    end

    subgraph "Chat Completion API"
        C1[ë¶„ë¥˜ Few-shot<br/>Classification Worker<br/>Gap < 0.05 ì‹œì—ë§Œ]
        C2[A1 ëˆ„ë½ ì¡°ë¬¸ ì¬ê²€ì¦<br/>MatchingVerifier]
        C3[A2 ì²´í¬ë¦¬ìŠ¤íŠ¸ ê²€ì¦<br/>ChecklistVerifier]
        C4[A3 ë‚´ìš© ë¹„êµ<br/>ContentComparator]
    end

    subgraph "Azure OpenAI"
        Azure[text-embedding-3-large<br/>3072 dim<br/>---<br/>gpt-4o<br/>JSON mode]
    end

    E1 --> Azure
    E2 --> Azure
    E3 --> Azure
    C1 --> Azure
    C2 --> Azure
    C3 --> Azure
    C4 --> Azure

    style E1 fill:#b3e5fc
    style E2 fill:#b3e5fc
    style E3 fill:#b3e5fc
    style C1 fill:#ffccbc
    style C2 fill:#ffccbc
    style C3 fill:#ffccbc
    style C4 fill:#ffccbc
    style Azure fill:#ffebee
```

### LLM í˜¸ì¶œ ìµœì í™”

| ë‹¨ê³„ | ìµœì í™” ê¸°ë²• | íš¨ê³¼ |
|-----|-----------|------|
| **ë¶„ë¥˜** | Hybrid Gating | LLM í˜¸ì¶œ ~60% ê°ì†Œ |
| **ì—…ë¡œë“œ** | ì„ë² ë”© ìºì‹± | ì¬ì—…ë¡œë“œ ì‹œ ì„ë² ë”© ì¬ì‚¬ìš© |
| **ê²€ì¦** | Sparse ì‹¤íŒ¨ ì‹œ Dense 100% | Whoosh ì˜¤ë¥˜ ì‹œ ë²¡í„° ê²€ìƒ‰ë§Œ ì‚¬ìš© |
| **í† í° ì¶”ì ** | TokenUsage í…Œì´ë¸” | ë¹„ìš© ëª¨ë‹ˆí„°ë§ ë° ë¶„ì„ |

### í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 

```python
# backend/shared/services/embedding_generator.py
def log_token_usage(contract_id, component, api_type, model, tokens):
    """
    component: classification_agent | consistency_agent
    api_type: chat_completion | embedding
    """
    TokenUsage.create(
        contract_id=contract_id,
        component=component,
        api_type=api_type,
        model=model,
        prompt_tokens=tokens["prompt_tokens"],
        completion_tokens=tokens["completion_tokens"],
        total_tokens=tokens["total_tokens"]
    )
```

**ì¬ì‹œë„ ë¡œì§**: SQLite ë½ ë°œìƒ ì‹œ 3íšŒ ì¬ì‹œë„ (ì§€ìˆ˜ ë°±ì˜¤í”„)

---

## ì£¼ìš” ê¸°ìˆ ì  íŠ¹ì§•

### 1. ì´ì¤‘ ë²¡í„° ì¸ë±ìŠ¤ (Dual Vector Index)

- **ê¸°ì¡´ ë¬¸ì œ**: ì œëª©ê³¼ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ì„ë² ë”©ìœ¼ë¡œ í•©ì¹˜ë©´ ì •ë³´ ì†ì‹¤
- **í•´ê²°ì±…**: ì œëª©ê³¼ ë‚´ìš©ì„ ë³„ë„ ì¸ë±ìŠ¤ë¡œ ë¶„ë¦¬
- **íš¨ê³¼**: ì œëª© ê¸°ë°˜ ë§¤ì¹­ ì •í™•ë„ í–¥ìƒ (íŠ¹íˆ ì§§ì€ ì¡°ë¬¸)

### 2. Hybrid Gating (ë¶„ë¥˜ ì—ì´ì „íŠ¸)

- **ê¸°ì¡´ ë¬¸ì œ**: ëª¨ë“  ë¶„ë¥˜ì— LLM ì‚¬ìš© ì‹œ ë¹„ìš© ê³¼ë‹¤
- **í•´ê²°ì±…**:
  - Gap >= 0.05: ì„ë² ë”© ê²°ê³¼ë§Œ ì‚¬ìš© (ë¹ ë¦„, ì €ë ´)
  - Gap < 0.05: LLM Few-shot í˜¸ì¶œ (ì •í™•, ë¹„ìŒˆ)
- **íš¨ê³¼**: ë¹„ìš© 60% ì ˆê°, ì‘ë‹µ ì†ë„ í–¥ìƒ

### 3. ì ì‘í˜• í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰

- **ê¸°ì¡´ ë¬¸ì œ**: Whoosh ì¸ë±ìŠ¤ ì˜¤ë¥˜ ì‹œ ì „ì²´ ê²€ìƒ‰ ì‹¤íŒ¨
- **í•´ê²°ì±…**: Sparse ê²°ê³¼ ì—†ìœ¼ë©´ Dense ê°€ì¤‘ì¹˜ 1.0ìœ¼ë¡œ ìë™ ì „í™˜
- **íš¨ê³¼**: ì‹œìŠ¤í…œ ì•ˆì •ì„± í–¥ìƒ

### 4. í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ (Mecab)

- **ê¸°ì¡´ ë¬¸ì œ**: ì˜ì–´ í† í¬ë‚˜ì´ì €ë¡œëŠ” í•œêµ­ì–´ ì˜ë¯¸ ì¶”ì¶œ ë¶ˆê°€
- **í•´ê²°ì±…**: Mecab í˜•íƒœì†Œ ë¶„ì„ê¸° + í’ˆì‚¬ í•„í„°ë§ (ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬)
- **íš¨ê³¼**: BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ

---

## êµ¬í˜„ ìƒíƒœ

| ì»´í¬ë„ŒíŠ¸ | ìƒíƒœ | ë¹„ê³  |
|---------|------|------|
| FastAPI Backend | âœ… ì™„ë£Œ | 10ê°œ ì—”ë“œí¬ì¸íŠ¸ |
| Streamlit Frontend | âœ… ì™„ë£Œ | ë‹¨ì¼ í˜ì´ì§€, Docker ë¯¸í¬í•¨ |
| Classification Worker | âœ… ì™„ë£Œ | Hybrid Gating ì ìš© |
| Consistency Worker | âœ… ì™„ë£Œ | A1, A2, A3 ë…¸ë“œ |
| Report Worker | âš ï¸ Stub | `{"status": "ok"}` ë°˜í™˜ë§Œ |
| ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶• | âœ… ì™„ë£Œ | 5ê°œ í‘œì¤€ ê³„ì•½ì„œ ì¸ë±ì‹± |
| í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ | âœ… ì™„ë£Œ | FAISS + Whoosh ì´ì¤‘ ì¸ë±ìŠ¤ |
| í† í° ì¶”ì  | âœ… ì™„ë£Œ | DB ì €ì¥ ë° API ì¡°íšŒ |
| ì¸ì¦/ê¶Œí•œ | âŒ ë¯¸êµ¬í˜„ | ë³´ì•ˆ ì—†ìŒ |
| CORS ì„¤ì • | âŒ ë¯¸êµ¬í˜„ | í”„ë¡ íŠ¸ì—”ë“œ í†µì‹  ì œí•œ ê°€ëŠ¥ |
| Docker Streamlit | âŒ ë¯¸êµ¬í˜„ | ìˆ˜ë™ ì‹¤í–‰ í•„ìš” |

---

## í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜

```bash
# Azure OpenAI
AZURE_OPENAI_API_KEY=your_api_key
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_GPT_DEPLOYMENT=gpt-4o
AZURE_EMBEDDING_DEPLOYMENT=text-embedding-3-large

# Redis
REDIS_URL=redis://redis:6379

# Database
DATABASE_URL=sqlite:///./data/database/contracts.db
```

### í¬íŠ¸ ë§¤í•‘

- **8000**: FastAPI (http://localhost:8000)
- **6379**: Redis (ë‚´ë¶€ ë„¤íŠ¸ì›Œí¬ë§Œ)
- **Streamlit**: Docker Composeì— ì—†ìŒ (ë³„ë„ ì‹¤í–‰)

### ì‹¤í–‰ ëª…ë ¹ì–´

```bash
# ì „ì²´ ì‹œìŠ¤í…œ ì‹œì‘
docker-compose up -d

# ì§€ì‹ ë² ì´ìŠ¤ êµ¬ì¶• (ìµœì´ˆ 1íšŒ)
docker-compose --profile ingestion run --rm ingestion run -m full -f all

# Streamlit ì‹¤í–‰ (ë³„ë„ í„°ë¯¸ë„)
cd frontend
streamlit run app.py

# ë¡œê·¸ í™•ì¸
docker-compose logs -f fast-api
docker-compose logs -f classification-worker
docker-compose logs -f consistency-validation-worker
```

---

## ì„±ëŠ¥ ì§€í‘œ

| í•­ëª© | ê°’ |
|-----|---|
| í‘œì¤€ ê³„ì•½ì„œ ì²­í¬ ìˆ˜ | ~400 (5ê°œ ê³„ì•½ì„œ í•©ê³„) |
| ê²€ìƒ‰ ì‘ë‹µ ì‹œê°„ | < 500ms |
| ë¶„ë¥˜ ì‹œê°„ (ì„ë² ë”©ë§Œ) | ~2ì´ˆ |
| ë¶„ë¥˜ ì‹œê°„ (LLM í¬í•¨) | ~5ì´ˆ |
| A1 ë…¸ë“œ ì‹¤í–‰ ì‹œê°„ | ~30ì´ˆ (50ê°œ ì¡°ë¬¸ ê°€ì •) |
| A2 ë…¸ë“œ ì‹¤í–‰ ì‹œê°„ | ~20ì´ˆ (20ê°œ ì²´í¬ë¦¬ìŠ¤íŠ¸ ê°€ì •) |
| A3 ë…¸ë“œ ì‹¤í–‰ ì‹œê°„ | ~60ì´ˆ (50ê°œ ì¡°ë¬¸ ê°€ì •) |
| ì „ì²´ ê²€ì¦ ì‹œê°„ | ~2ë¶„ |

---

## ì°¸ê³  ë¬¸ì„œ

- [í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë¡œì§](./HYBRID_SEARCH_LOGIC.md)
- [A1 ë…¸ë“œ ë§¤ì¹­ í”Œë¡œìš°](./A1_SEARCH_MATCHING_FLOW.md)
- [í”„ë¡œì íŠ¸ ìƒíƒœ](../PROJECT_STATUS.md)
- [ê¸°ìˆ  ìŠ¤íƒ](./.kiro/steering/tech.md)
- [ì œí’ˆ ê°œìš”](./.kiro/steering/product.md)

---

## ë³€ê²½ ì´ë ¥

- **2025-11-03**: ì´ˆê¸° ì‘ì„± (ì‹¤ì œ ì½”ë“œë² ì´ìŠ¤ ë¶„ì„ ê¸°ë°˜)
  - ì´ì¤‘ ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¡° ë°˜ì˜
  - Hybrid Gating ìµœì í™” ë°˜ì˜
  - A1/A2/A3 ë…¸ë“œ ì‹¤ì œ êµ¬í˜„ ìƒíƒœ ë°˜ì˜
  - Report Worker stub ìƒíƒœ ëª…ì‹œ
>>>>>>> c7c7d8f082d58f15a66cf160f5d601f1ab908b93
